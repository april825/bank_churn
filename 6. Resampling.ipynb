{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "federal-front",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:rgb(67, 77, 86);\n",
    "           font-size:300%;\n",
    "           font-style: oblique;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 20px;\">Predicting Bank Churners</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-occupation",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "<h2 style=\"background-color:rgb(141, 153, 165);\n",
    "           font-size:250%;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 10px;\">Chapter 6. Resampling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-architect",
   "metadata": {},
   "source": [
    "<a id='1.1'>\n",
    "    <h2 style='font-size:180%;'>\n",
    "        Mission</h2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-telephone",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <blockquote cite='https://www.kaggle.com/sakshigoyal7/credit-card-customers/tasks?taskId=2729'>\n",
    "        <p style='font-size:110%;\n",
    "                  color:hsl(208, 12%, 30%);'><i>Our top priority in this business problem is to identify customers who are getting churned. Even if we predict non-churning customers as churned, it won't harm our business. But predicting churning customers as non-churning will do. So recall needs to be higher. Till now, I have managed to get a recall of 62%.</i></p>\n",
    "    </blockquote>\n",
    "    <figcaption>â€”Sakshi Goyal, <cite>Credit Card Customers, Kaggle</cite></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-alfred",
   "metadata": {},
   "source": [
    "<a id='4.1'>\n",
    "    <h2 style='font-size:180%;'>\n",
    "        Libraries</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-advertiser",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# statistics\n",
    "from numpy import (mean, std)\n",
    "from scipy.stats import (\n",
    "    pearsonr, spearmanr, kendalltau,\n",
    "    chi2_contingency, f_oneway)\n",
    "\n",
    "# machine learning prep\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler, RobustScaler, QuantileTransformer, PowerTransformer)\n",
    "from sklearn.feature_selection import RFE\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_validate, cross_val_predict,\n",
    "    RepeatedStratifiedKFold, GridSearchCV, RandomizedSearchCV)\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, fbeta_score, auc, roc_auc_score,\n",
    "    precision_recall_curve, plot_precision_recall_curve, average_precision_score, precision_recall_fscore_support,\n",
    "    classification_report, precision_recall_fscore_support, confusion_matrix, SCORERS, make_scorer)\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import (SVC, LinearSVC) # remove SVC later if not used\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, BaggingClassifier, \n",
    "    GradientBoostingClassifier, IsolationForest)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import imblearn\n",
    "\n",
    "# warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "# warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "\n",
    "# saving\n",
    "import os\n",
    "\n",
    "# efficiency\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-collaboration",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-classics",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* CSS styles for pandas dataframe */\n",
    ".dataframe th {\n",
    "    font-size: 16px;\n",
    "}\n",
    ".dataframe td {\n",
    "    font-size: 14px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-religious",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.set_options('precision', 3)\n",
    "# pd.set_options('min_rows', 6)\n",
    "# pd.set_options('max_rows', 10)\n",
    "# pd.reset_option('max_rows')\n",
    "# pd.set_option('max_colwidth', 10)\n",
    "# pd.set_option(\"chop_threshold\", 0.5)\n",
    "# pd.reset_option(\"chop_threshold\")\n",
    "# pd.set_option(\"colheader_justify\", \"left\")\n",
    "# pd.reset_option(\"colheader_justify\")\n",
    "# plt.rc('figure',figsize=(8,4))\n",
    "# plt.style.use('seaborn-whitegrid')\n",
    "# from IPython.display import display, Math, Latex\n",
    "# pio.renderers.default='plotly_mimetype'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-encyclopedia",
   "metadata": {},
   "source": [
    "<a id='4.2'>\n",
    "    <h2 style='font-size:180%;'>\n",
    "        Data Loading</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-mixer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_normal = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-composer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "d = pd.read_csv('source/d_num.csv')\n",
    "d.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-setup",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_values = d.values\n",
    "x, y = d_values[:,1:], d_values[:,:1].ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-salem",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "<h2 style=\"background-color:rgb(141, 153, 165);\n",
    "           font-size:250%;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 10px;\">Functions to Use</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-intellectual",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-processor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perf_metrics(y_test, y_pred):\n",
    "    dic = {}\n",
    "    dic['accuracy'] = round(accuracy_score(y_test, y_pred), 2)\n",
    "    dic['precision'] = round(precision_score(y_test, y_pred), 2)\n",
    "    dic['recall'] = round(recall_score(y_test, y_pred), 2)\n",
    "    dic['f1'] = round(f1_score(y_test, y_pred), 2)\n",
    "    dic['f2'] = round(fbeta_score(y_test, y_pred, beta=2), 2)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-garage",
   "metadata": {},
   "source": [
    "### Result Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-stewart",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def result_rskf(x, y, pipeline, mod_disp_name, n_splits=5, n_repeats=3):\n",
    "   \n",
    "    # define cv method\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits, n_repeats=n_repeats, random_state=1)  \n",
    "    \n",
    "    # define performance metrics\n",
    "    scoring = {\n",
    "        'accuracy':'accuracy', 'precision':'precision', 'recall':'recall', 'f1':'f1', \n",
    "        'f2':make_scorer(fbeta_score, beta=2)} # dict val = scorer fct or predefined metric str  \n",
    "    \n",
    "    # evaluate result\n",
    "    result = cross_validate(\n",
    "        pipeline, x, y, cv=cv, \n",
    "        scoring=scoring, return_train_score=True, n_jobs=-1)\n",
    "        \n",
    "    # make a summary table\n",
    "    df = pd.DataFrame(\n",
    "        (k, mean(v), std(v)) for k,v in result.items()\n",
    "        ).rename({0:'metric', 1:'mean', 2:'std'}, axis=1\n",
    "                ).set_index('metric')\n",
    "    df.index.name = None\n",
    "    df.columns = pd.MultiIndex.from_product([[mod_disp_name],df.columns])\n",
    "    \n",
    "    return df, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-qatar",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summary_by_mod(x, y, models, scalers, result_func=result_rskf, **n_splits_and_repeats):\n",
    "    results = []\n",
    "    time_0 = time.time() # for all methods in pipeline\n",
    "    for scaler in scalers:\n",
    "        results_models = []\n",
    "        time_1 = time.time() # for each scaler\n",
    "        print(f'Scaler: {scaler[0]}\\n')\n",
    "        for model in models:\n",
    "            time_2 = time.time() # for each model\n",
    "            pipeline = Pipeline([('s', scaler[1]), ('m', model[1])])\n",
    "            if result_func==result_rskf:\n",
    "                n_splits, n_repeats = (i for i in n_splits_and_repeats.values())\n",
    "                results_model = result_func(x, y, pipeline, model[0], n_splits, n_repeats)[0]\n",
    "            else:\n",
    "                results_model = result_func(x, y, pipeline, model[0])[0]\n",
    "            print(f'Model {model[0]} Runtime: {time.strftime(\"%M:%S\", time.gmtime(time.time()-time_2))}')\n",
    "            results_models.append(results_model)\n",
    "        print(f'Scaler {scaler[0]} Avg Runtime per Model: {time.strftime(\"%M:%S\", time.gmtime((time.time()-time_1)/len(models)))}\\n\\n')\n",
    "        results.append(results_models)\n",
    "    print(f'Total Runtime: {time.strftime(\"%M:%S\", time.gmtime(time.time()-time_0))} min')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-chancellor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summary_by_mod2(pred_mods, scalers, features, **n_splits_and_repeats):\n",
    "    \n",
    "    time_0 = time.time() # for all methods in pipeline\n",
    "    \n",
    "    results = []\n",
    "    for scaler in scalers:\n",
    "        time_1 = time.time() # for each scaler\n",
    "        print(f'\\nScaler: {scaler[0]}')\n",
    "        \n",
    "        results_features = []\n",
    "        for feature in features:\n",
    "            time_2 = time.time() # for each feature selection model\n",
    "            print(f'\\nFS Model: {feature[0]}')\n",
    "            \n",
    "            results_pred_mods = []\n",
    "            for pred_mod in pred_mods:\n",
    "                time_3 = time.time() # for each prediction model\n",
    "                \n",
    "                # define pipeline\n",
    "                pipeline = Pipeline([('s', scaler[1]), ('fs', feature[1]), ('m', pred_mod[1])])\n",
    "\n",
    "                # fit models\n",
    "                n_splits, n_repeats = (i for i in n_splits_and_repeats.values())\n",
    "                results_model = result_rskf(x, y, pipeline, pred_mod[0], n_splits, n_repeats)[0]\n",
    "                \n",
    "                # collect results - innermost\n",
    "                print(f'Model {pred_mod[0]} Runtime: {time.strftime(\"%M:%S\", time.gmtime(time.time()-time_3))}')\n",
    "                results_pred_mods.append(results_model)\n",
    "            \n",
    "            # collect results - middle\n",
    "            print(f'\\nFS Model {feature[0]} Avg Runtime per Model: {time.strftime(\"%M:%S\", time.gmtime((time.time()-time_2)/len(pred_mods)))}\\n')\n",
    "            results_features.append(results_pred_mods)\n",
    "        \n",
    "        # collect results - outermost\n",
    "        print(f'Scaler {scaler[0]} Avg Runtime per Model: {time.strftime(\"%M:%S\", time.gmtime((time.time()-time_1)/len(features)))}\\n\\n')\n",
    "        results.append(results_features)\n",
    "        \n",
    "    print(f'Total Runtime: {time.strftime(\"%M:%S\", time.gmtime(time.time()-time_0))} min')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-mixture",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "<h2 style=\"background-color:rgb(141, 153, 165);\n",
    "           font-size:250%;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 10px;\">Benchmark</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-office",
   "metadata": {},
   "source": [
    "# Define Models & Pre-Processing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-freeze",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples for all models to explore: [(`model name`, `model instance`)] with minimum hyperparameter setting\n",
    "models = []\n",
    "\n",
    "# linear\n",
    "models.append(('LR', LogisticRegression(solver='saga', max_iter=1000, class_weight='balanced', random_state=5))) # note: `max_iter` from 1000 to 10000 due to convergence issues\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "\n",
    "# non-linear\n",
    "models.append(('DT', DecisionTreeClassifier(random_state=5)))\n",
    "models.append(('KNN', KNeighborsClassifier(n_neighbors=5)))\n",
    "models.append(('MLP', MLPClassifier(max_iter=5000, random_state=5)))\n",
    "\n",
    "# ensemble\n",
    "models.append(('BDT', BaggingClassifier(n_estimators=100, n_jobs=-1, random_state=5)))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=5))) # note: increasing n_estimators more than 400 doesn't do much; some in place to prevent too much overfitting\n",
    "models.append(('GB', GradientBoostingClassifier(max_depth=10, random_state=5))) # note: `max_iter` from 100 to 1000 due to convergence issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-programming",
   "metadata": {},
   "source": [
    "## Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples for all scalers to explore: [(`scaler name`, `scaler instance`)]\n",
    "scalers = []\n",
    "scalers.append(('RS', RobustScaler()))\n",
    "scalers.append(('QT', QuantileTransformer()))\n",
    "scalers.append(('MM', MinMaxScaler()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-receipt",
   "metadata": {},
   "source": [
    "## Resamplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_smote = SMOTE(random_state=5, n_jobs=-1)\n",
    "over_smote_nc = SMOTENC(random_state=5, n_jobs=-1) # Over-sample using SMOTE for continuous and categorical features.\n",
    "over_smote_bl = BorderlineSMOTE(random_state=5, n_jobs=-1) # Over-sample using the borderline-SMOTE variant.\n",
    "over_smote_km = KMeansSMOTE(random_state=5, n_jobs=-1) # Over-sample applying a clustering before to oversample using SMOTE.\n",
    "over_smote_svm = SVMSMOTE(random_state=5, n_jobs=-1) # Over-sample using the SVM-SMOTE variant.\n",
    "over_adasyn = ADASYN(random_state=5, n_jobs=-1) # Over-sample using ADASYN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples for all resampling models to explore: [(`fs name`, `fs instance`)]\n",
    "resample = []\n",
    "resample.append(('SMOTE_ORIG', over_smote))\n",
    "resample.append(('SMOTE_NC', over_smote_nc))\n",
    "resample.append(('SMOTE_BL', over_smote_bl))\n",
    "resample.append(('SMOTE_KM', over_smote_km))\n",
    "resample.append(('SMOTE_SVM', over_smote_svm))\n",
    "resample.append(('ADASYN', over_adasyn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-secretariat",
   "metadata": {},
   "source": [
    "## Feature Selectors/Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples for all feature selection/extraction models to explore: [(`fs name`, `fs instance`)]\n",
    "features = []\n",
    "features.append(('RFE', RFE(estimator=GradientBoostingClassifier(max_depth=10, random_state=5), n_features_to_select=20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-expression",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "d_values = d.values\n",
    "x, y = d_values[:,1:], d_values[:,:1].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "scaler = RobustScaler()\n",
    "model = GradientBoostingClassifier(random_state=5)\n",
    "steps = [('scaler', scaler), ('model', model)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=5)\n",
    "scoring = {'accuracy':'accuracy', 'precision':'precision', 'recall':'recall', 'f1':'f1',\n",
    "           'f2':make_scorer(fbeta_score, beta=2)} \n",
    "result = cross_validate(\n",
    "    pipeline, x, y, cv=cv, \n",
    "    scoring=scoring, return_train_score=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a summary table\n",
    "df = pd.DataFrame(\n",
    "    (k, mean(v), std(v)) for k,v in result.items()\n",
    "    ).rename({0:'metric', 1:'mean', 2:'std'}, axis=1\n",
    "            ).set_index('metric')\n",
    "# df.index.name = None\n",
    "# df.columns = pd.MultiIndex.from_product([[mod_disp_name],df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "scaler = RobustScaler()\n",
    "over = SMOTE()\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "model = GradientBoostingClassifier(random_state=5)\n",
    "steps = [('over', over), ('scaler', scaler), ('model', model)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=5)\n",
    "scoring = {'accuracy':'accuracy', 'precision':'precision', 'recall':'recall', 'f1':'f1',\n",
    "           'f2':make_scorer(fbeta_score, beta=2)} # dict val = scorer fct or predefined metric str  \n",
    "result = cross_validate(\n",
    "    pipeline, x, y, cv=cv, \n",
    "    scoring=scoring, return_train_score=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a summary table\n",
    "df = pd.DataFrame(\n",
    "    (k, mean(v), std(v)) for k,v in result.items()\n",
    "    ).rename({0:'metric', 1:'mean', 2:'std'}, axis=1\n",
    "            ).set_index('metric')\n",
    "# df.index.name = None\n",
    "# df.columns = pd.MultiIndex.from_product([[mod_disp_name],df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "scaler = RobustScaler()\n",
    "over = SMOTE(random_state=5, n_jobs=-1)\n",
    "under = RandomUnderSampler()\n",
    "model = GradientBoostingClassifier(random_state=5)\n",
    "steps = [('over', over), ('under', under), ('scaler', scaler), ('model', model)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=5)\n",
    "scoring = {'accuracy':'accuracy', 'precision':'precision', 'recall':'recall', 'f1':'f1',\n",
    "           'f2':make_scorer(fbeta_score, beta=2)} # dict val = scorer fct or predefined metric str  \n",
    "result = cross_validate(\n",
    "    pipeline, x, y, cv=cv, \n",
    "    scoring=scoring, return_train_score=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a summary table\n",
    "df = pd.DataFrame(\n",
    "    (k, mean(v), std(v)) for k,v in result.items()\n",
    "    ).rename({0:'metric', 1:'mean', 2:'std'}, axis=1\n",
    "            ).set_index('metric')\n",
    "# df.index.name = None\n",
    "# df.columns = pd.MultiIndex.from_product([[mod_disp_name],df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-threshold",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-arthritis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTE().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-endorsement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-spell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-alcohol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "native-scanning",
   "metadata": {},
   "source": [
    "<a id='4.2'>\n",
    "    <h2 style='font-size:180%;'>\n",
    "        Validation Set</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-screening",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_rfe = summary_by_mod2(models, scalers, features, n_splits=5, n_repeats=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-detective",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_rfe_RS = pd.concat([i for i in results_rfe[0][0]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-measure",
   "metadata": {},
   "source": [
    "<a id='4.2'>\n",
    "    <h2 style='font-size:150%;'>\n",
    "        Summary Treatment</h2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-shaft",
   "metadata": {},
   "source": [
    "Again, results are far worse than the baseline, but we may revisit after resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-monkey",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Results for Robust Scaler:')\n",
    "tem = results_rfe_RS.loc[['test_recall', 'test_precision', 'test_f2']]\n",
    "df_RS_rfe_RS_summ = tem.loc[:,np.in1d(tem.columns.get_level_values(1), 'mean')].droplevel(level=1, axis=1).rename(\n",
    "    index={'test_recall':'recall', 'test_precision':'prec', 'test_f2':'f2'}).T\n",
    "df_RS_rfe_RS_summ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
