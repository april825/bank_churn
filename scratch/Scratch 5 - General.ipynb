{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stuffed-spank",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:rgb(67, 77, 86);\n",
    "           font-size:300%;\n",
    "           font-style: oblique;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 20px;\">Predicting Bank Churners</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-basis",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "<h2 style=\"background-color:rgb(141, 153, 165);\n",
    "           font-size:250%;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 10px;\">Chapter 5. Spot Check Version 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-wells",
   "metadata": {},
   "source": [
    "<a id='1.1'>\n",
    "    <h2 style='font-size:180%;'>\n",
    "        Mission</h2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-spirit",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <blockquote cite='https://www.kaggle.com/sakshigoyal7/credit-card-customers/tasks?taskId=2729'>\n",
    "        <p style='font-size:110%;\n",
    "                  color:hsl(208, 12%, 30%);'><i>Our top priority in this business problem is to identify customers who are getting churned. Even if we predict non-churning customers as churned, it won't harm our business. But predicting churning customers as non-churning will do. So recall needs to be higher. Till now, I have managed to get a recall of 62%.</i></p>\n",
    "    </blockquote>\n",
    "    <figcaption>â€”Sakshi Goyal, <cite>Credit Card Customers, Kaggle</cite></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-passage",
   "metadata": {},
   "source": [
    "<a id='4.1'>\n",
    "    <h2 style='font-size:180%;'>\n",
    "        Libraries</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-contrary",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# statistics\n",
    "from numpy import (mean, std)\n",
    "from scipy.stats import (\n",
    "    pearsonr, spearmanr, kendalltau,\n",
    "    chi2_contingency, f_oneway)\n",
    "\n",
    "# machine learning prep\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler, RobustScaler, QuantileTransformer, PowerTransformer)\n",
    "from sklearn.feature_selection import RFE\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_validate, cross_val_predict,\n",
    "    RepeatedStratifiedKFold, GridSearchCV, RandomizedSearchCV)\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, fbeta_score, auc, roc_auc_score,\n",
    "    precision_recall_curve, plot_precision_recall_curve, average_precision_score, precision_recall_fscore_support,\n",
    "    classification_report, precision_recall_fscore_support, confusion_matrix, SCORERS, make_scorer)\n",
    "\n",
    "\n",
    "# from sklearn.pipeline import Pipeline\n",
    "import imblearn.pipeline \n",
    "from imblearn import Pipeline, make_pipeline\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import (SVC, LinearSVC) # remove SVC later if not used\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, BaggingClassifier, \n",
    "    GradientBoostingClassifier, IsolationForest)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "# warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "\n",
    "# saving\n",
    "import os\n",
    "\n",
    "# efficiency\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-waterproof",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-corporation",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* CSS styles for pandas dataframe */\n",
    ".dataframe th {\n",
    "    font-size: 16px;\n",
    "}\n",
    ".dataframe td {\n",
    "    font-size: 14px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-general",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.set_options('precision', 3)\n",
    "# pd.set_options('min_rows', 6)\n",
    "# pd.set_options('max_rows', 10)\n",
    "# pd.reset_option('max_rows')\n",
    "# pd.set_option('max_colwidth', 10)\n",
    "# pd.set_option(\"chop_threshold\", 0.5)\n",
    "# pd.reset_option(\"chop_threshold\")\n",
    "# pd.set_option(\"colheader_justify\", \"left\")\n",
    "# pd.reset_option(\"colheader_justify\")\n",
    "# plt.rc('figure',figsize=(8,4))\n",
    "# plt.style.use('seaborn-whitegrid')\n",
    "# from IPython.display import display, Math, Latex\n",
    "# pio.renderers.default='plotly_mimetype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-rates",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_normal = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-given",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "<h2 style=\"background-color:rgb(141, 153, 165);\n",
    "           font-size:250%;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 10px;\">Model & Pipeline Set-Up</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-highland",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-gregory",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of tuples for all models to explore: [(`model name`, `model instance`)] with minimum hyperparameter setting\n",
    "models = []\n",
    "\n",
    "# linear\n",
    "models.append(('LR', LogisticRegression(solver='saga', max_iter=1000, class_weight='balanced', random_state=5))) # note: `max_iter` from 1000 to 10000 due to convergence issues\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "\n",
    "# non-linear\n",
    "models.append(('DT', DecisionTreeClassifier(random_state=5)))\n",
    "models.append(('KNN', KNeighborsClassifier(n_neighbors=5)))\n",
    "models.append(('MLP', MLPClassifier(max_iter=5000, random_state=5)))\n",
    "\n",
    "# ensemble\n",
    "models.append(('BDT', BaggingClassifier(n_estimators=100, n_jobs=-1, random_state=5)))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=5))) # note: increasing n_estimators more than 400 doesn't do much; some in place to prevent too much overfitting\n",
    "models.append(('GB', GradientBoostingClassifier(max_depth=10, random_state=5))) # note: `max_iter` from 100 to 1000 due to convergence issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-composer",
   "metadata": {},
   "source": [
    "## Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-ability",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of tuples for all scalers to explore: [(`scaler name`, `scaler instance`)]\n",
    "scalers = []\n",
    "scalers.append(('RS', RobustScaler()))\n",
    "scalers.append(('QT', QuantileTransformer()))\n",
    "scalers.append(('MM', MinMaxScaler()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-green",
   "metadata": {},
   "source": [
    "## Resamplers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-deadline",
   "metadata": {},
   "source": [
    "[Source: How to Combine Oversampling and Undersampling for Imbalanced Classification, *Machine Learning Mastery*](https://machinelearningmastery.com/combine-oversampling-and-undersampling-for-imbalanced-classification/)\n",
    "<br>[Source: Undersampling Algorithms for Imbalanced Classification, *Machine Learning Mastery*](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-blues",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of tuples for all resampling models to explore: [(`fs name`, `fs instance`)]\n",
    "\n",
    "## oversampling\n",
    "over_rd = RandomOverSampler(sampling_strategy=0.16)\n",
    "over_smote = SMOTE(random_state=5, n_jobs=-1)\n",
    "over_smote_nc = SMOTENC(random_state=5, n_jobs=-1) # Over-sample using SMOTE for continuous and categorical features.\n",
    "over_smote_bl = BorderlineSMOTE(random_state=5, n_jobs=-1) # Over-sample using the borderline-SMOTE variant.\n",
    "over_smote_km = KMeansSMOTE(random_state=5, n_jobs=-1) # Over-sample applying a clustering before to oversample using SMOTE.\n",
    "over_smote_svm = SVMSMOTE(random_state=5, n_jobs=-1) # Over-sample using the SVM-SMOTE variant.\n",
    "over_adasyn = ADASYN(random_state=5, n_jobs=-1) # Over-sample using ADASYN.\n",
    "\n",
    "resample = []\n",
    "\n",
    "resample.append(('OVER_RD', over_rd))\n",
    "resample.append(('SM_ORIG', over_smote))\n",
    "resample.append(('SM_NC', over_smote_nc))\n",
    "resample.append(('SM_BL', over_smote_bl))\n",
    "resample.append(('SM_KM', over_smote_km))\n",
    "resample.append(('SM_SVM', over_smote_svm))\n",
    "resample.append(('ADASYN', over_adasyn))\n",
    "\n",
    "## undersampling\n",
    "under_rd = RandomUnderSampler(sampling_strategy=0.5)\n",
    "# NearMiss-1: Majority class examples with minimum average distance to three closest minority class examples.\n",
    "# NearMiss-2: Majority class examples with minimum average distance to three furthest minority class examples.\n",
    "# NearMiss-3: Majority class examples with minimum distance to each minority class example.\n",
    "undersample = NearMiss(version=1, n_neighbors=3)\n",
    "undersample = NearMiss(version=2, n_neighbors=3)\n",
    "undersample = NearMiss(version=3, n_neighbors=3)\n",
    "undersample = CondensedNearestNeighbour(n_neighbors=1) # slow + random selection\n",
    "undersample = TomekLinks()\n",
    "undersample = EditedNearestNeighbours(n_neighbors=3)\n",
    "\n",
    "resample.append(('UNDER_RD', under_rd))\n",
    "\n",
    "## combination\n",
    "combi_smote_tomek = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority')) # SMOTE and Tomek Links\n",
    "combi_smote_enn = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority')) # SMOTE and Edited Nearest Neighbors\n",
    "\n",
    "resample.append(('SMTOM', combi_smote_tomek))\n",
    "resample.append(('SMENN', combi_smote_tomek))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-judges",
   "metadata": {},
   "source": [
    "## Feature Selectors/Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-watershed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a list of tuples for all feature selection/extraction models to explore: [(`fs name`, `fs instance`)]\n",
    "features = []\n",
    "features.append(('RFE', RFE(estimator=GradientBoostingClassifier(max_depth=10, random_state=5), n_features_to_select=20)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
