{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "planned-virginia",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification spot check script\n",
    "import warnings\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-parallel",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, returns X and y elements\n",
    "def load_dataset():\n",
    "\treturn make_classification(n_samples=1000, n_classes=2, random_state=1)\n",
    "\n",
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "\t# linear models\n",
    "\tmodels['logistic'] = LogisticRegression()\n",
    "\talpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\tfor a in alpha:\n",
    "\t\tmodels['ridge-'+str(a)] = RidgeClassifier(alpha=a)\n",
    "\tmodels['sgd'] = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "\tmodels['pa'] = PassiveAggressiveClassifier(max_iter=1000, tol=1e-3)\n",
    "\t# non-linear models\n",
    "\tn_neighbors = range(1, 21)\n",
    "\tfor k in n_neighbors:\n",
    "\t\tmodels['knn-'+str(k)] = KNeighborsClassifier(n_neighbors=k)\n",
    "\tmodels['cart'] = DecisionTreeClassifier()\n",
    "\tmodels['extra'] = ExtraTreeClassifier()\n",
    "\tmodels['svml'] = SVC(kernel='linear')\n",
    "\tmodels['svmp'] = SVC(kernel='poly')\n",
    "\tc_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\tfor c in c_values:\n",
    "\t\tmodels['svmr'+str(c)] = SVC(C=c)\n",
    "\tmodels['bayes'] = GaussianNB()\n",
    "\t# ensemble models\n",
    "\tn_trees = 100\n",
    "\tmodels['ada'] = AdaBoostClassifier(n_estimators=n_trees)\n",
    "\tmodels['bag'] = BaggingClassifier(n_estimators=n_trees)\n",
    "\tmodels['rf'] = RandomForestClassifier(n_estimators=n_trees)\n",
    "\tmodels['et'] = ExtraTreesClassifier(n_estimators=n_trees)\n",
    "\tmodels['gbm'] = GradientBoostingClassifier(n_estimators=n_trees)\n",
    "\tprint('Defined %d models' % len(models))\n",
    "\treturn models\n",
    "\n",
    "# no transforms pipeline\n",
    "def pipeline_none(model):\n",
    "\treturn model\n",
    "\n",
    "# standardize transform pipeline\n",
    "def pipeline_standardize(model):\n",
    "\tsteps = list()\n",
    "\t# standardization\n",
    "\tsteps.append(('standardize', StandardScaler()))\n",
    "\t# the model\n",
    "\tsteps.append(('model', model))\n",
    "\t# create pipeline\n",
    "\tpipeline = Pipeline(steps=steps)\n",
    "\treturn pipeline\n",
    "\n",
    "# normalize transform pipeline\n",
    "def pipeline_normalize(model):\n",
    "\tsteps = list()\n",
    "\t# normalization\n",
    "\tsteps.append(('normalize', MinMaxScaler()))\n",
    "\t# the model\n",
    "\tsteps.append(('model', model))\n",
    "\t# create pipeline\n",
    "\tpipeline = Pipeline(steps=steps)\n",
    "\treturn pipeline\n",
    "\n",
    "# standardize and normalize pipeline\n",
    "def pipeline_std_norm(model):\n",
    "\tsteps = list()\n",
    "\t# standardization\n",
    "\tsteps.append(('standardize', StandardScaler()))\n",
    "\t# normalization\n",
    "\tsteps.append(('normalize', MinMaxScaler()))\n",
    "\t# the model\n",
    "\tsteps.append(('model', model))\n",
    "\t# create pipeline\n",
    "\tpipeline = Pipeline(steps=steps)\n",
    "\treturn pipeline\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(X, y, model, folds, metric, pipe_func):\n",
    "\t# create the pipeline\n",
    "\tpipeline = pipe_func(model)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(pipeline, X, y, scoring=metric, cv=folds, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# evaluate a model and try to trap errors and and hide warnings\n",
    "def robust_evaluate_model(X, y, model, folds, metric, pipe_func):\n",
    "\tscores = None\n",
    "\ttry:\n",
    "\t\twith warnings.catch_warnings():\n",
    "\t\t\twarnings.filterwarnings(\"ignore\")\n",
    "\t\t\tscores = evaluate_model(X, y, model, folds, metric, pipe_func)\n",
    "\texcept:\n",
    "\t\tscores = None\n",
    "\treturn scores\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(X, y, models, pipe_funcs, folds=10, metric='accuracy'):\n",
    "\tresults = dict()\n",
    "\tfor name, model in models.items():\n",
    "\t\t# evaluate model under each preparation function\n",
    "\t\tfor i in range(len(pipe_funcs)):\n",
    "\t\t\t# evaluate the model\n",
    "\t\t\tscores = robust_evaluate_model(X, y, model, folds, metric, pipe_funcs[i])\n",
    "\t\t\t# update name\n",
    "\t\t\trun_name = str(i) + name\n",
    "\t\t\t# show process\n",
    "\t\t\tif scores is not None:\n",
    "\t\t\t\t# store a result\n",
    "\t\t\t\tresults[run_name] = scores\n",
    "\t\t\t\tmean_score, std_score = mean(scores), std(scores)\n",
    "\t\t\t\tprint('>%s: %.3f (+/-%.3f)' % (run_name, mean_score, std_score))\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint('>%s: error' % run_name)\n",
    "\treturn results\n",
    "\n",
    "# print and plot the top n results\n",
    "def summarize_results(results, maximize=True, top_n=10):\n",
    "\t# check for no results\n",
    "\tif len(results) == 0:\n",
    "\t\tprint('no results')\n",
    "\t\treturn\n",
    "\t# determine how many results to summarize\n",
    "\tn = min(top_n, len(results))\n",
    "\t# create a list of (name, mean(scores)) tuples\n",
    "\tmean_scores = [(k,mean(v)) for k,v in results.items()]\n",
    "\t# sort tuples by mean score\n",
    "\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "\t# reverse for descending order (e.g. for accuracy)\n",
    "\tif maximize:\n",
    "\t\tmean_scores = list(reversed(mean_scores))\n",
    "\t# retrieve the top n for summarization\n",
    "\tnames = [x[0] for x in mean_scores[:n]]\n",
    "\tscores = [results[x[0]] for x in mean_scores[:n]]\n",
    "\t# print the top n\n",
    "\tprint()\n",
    "\tfor i in range(n):\n",
    "\t\tname = names[i]\n",
    "\t\tmean_score, std_score = mean(results[name]), std(results[name])\n",
    "\t\tprint('Rank=%d, Name=%s, Score=%.3f (+/- %.3f)' % (i+1, name, mean_score, std_score))\n",
    "\t# boxplot for the top n\n",
    "\tpyplot.boxplot(scores, labels=names)\n",
    "\t_, labels = pyplot.xticks()\n",
    "\tpyplot.setp(labels, rotation=90)\n",
    "\tpyplot.savefig('spotcheck.png')\n",
    "\n",
    "# load dataset\n",
    "X, y = load_dataset()\n",
    "# get model list\n",
    "models = define_models()\n",
    "# define transform pipelines\n",
    "pipelines = [pipeline_none, pipeline_standardize, pipeline_normalize, pipeline_std_norm]\n",
    "# evaluate models\n",
    "results = evaluate_models(X, y, models, pipelines)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-bennett",
   "metadata": {},
   "source": [
    "# Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, returns X and y elements\n",
    "def load_dataset():\n",
    "    d = pd.read_csv('source/d_num.csv')\n",
    "    d_values = d.values\n",
    "    return d_values[:,1:], d_values[:,:1].ravel()\n",
    "\n",
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "    # linear models\n",
    "     \n",
    "    # non-linear model\n",
    "#     iter_times = [500, 1000, 2000]\n",
    "#     for i in iter_times:\n",
    "#         models['MLP'] = MLPClassifier(max_iter=i, random_state=5)\n",
    "#     models['cart'] = DecisionTreeClassifier()\n",
    "#     models['extra'] = ExtraTreeClassifier()\n",
    "#     models['svmp'] = SVC(kernel='poly')\n",
    "#     c_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "#     for c in c_values:\n",
    "#         models['svmr'+str(c)] = SVC(C=c)\n",
    "        \n",
    "    # ensemble models\n",
    "    n_trees = [10, 30, 50 , 100]\n",
    "    models['ada'] = AdaBoostClassifier(n_estimators=n_trees, random_state=5)\n",
    "    models['bag'] = BaggingClassifier(n_estimators=n_trees, n_jobs=-1, random_state=5)\n",
    "    models['rf'] = RandomForestClassifier(n_estimators=n_trees, n_jobs=-1, random_state=5)\n",
    "    models['et'] = ExtraTreesClassifier(n_estimators=n_trees, n_jobs=-1, random_state=5)\n",
    "    models['gbm'] = GradientBoostingClassifier(n_estimators=n_trees, random_state=5)\n",
    "    print('Defined %d models' % len(models))\n",
    "    return models\n",
    "\n",
    "# no transforms pipeline\n",
    "def pipeline_none(model):\n",
    "\treturn model\n",
    "\n",
    "# standardize transform pipeline\n",
    "def pipeline_standardize(model):\n",
    "\tsteps = list()\n",
    "\t# standardization\n",
    "\tsteps.append(('standardize', StandardScaler()))\n",
    "\t# the model\n",
    "\tsteps.append(('model', model))\n",
    "\t# create pipeline\n",
    "\tpipeline = Pipeline(steps=steps)\n",
    "\treturn pipeline\n",
    "\n",
    "# normalize transform pipeline\n",
    "def pipeline_normalize(model):\n",
    "\tsteps = list()\n",
    "\t# normalization\n",
    "\tsteps.append(('normalize', MinMaxScaler()))\n",
    "\t# the model\n",
    "\tsteps.append(('model', model))\n",
    "\t# create pipeline\n",
    "\tpipeline = Pipeline(steps=steps)\n",
    "\treturn pipeline\n",
    "\n",
    "# standardize and normalize pipeline\n",
    "def pipeline_std_norm(model):\n",
    "\tsteps = list()\n",
    "\t# standardization\n",
    "\tsteps.append(('standardize', StandardScaler()))\n",
    "\t# normalization\n",
    "\tsteps.append(('normalize', MinMaxScaler()))\n",
    "\t# the model\n",
    "\tsteps.append(('model', model))\n",
    "\t# create pipeline\n",
    "\tpipeline = Pipeline(steps=steps)\n",
    "\treturn pipeline\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(X, y, model, folds, metric, pipe_func):\n",
    "\t# create the pipeline\n",
    "\tpipeline = pipe_func(model)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(pipeline, X, y, scoring=metric, cv=folds, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# evaluate a model and try to trap errors and and hide warnings\n",
    "def robust_evaluate_model(X, y, model, folds, metric, pipe_func):\n",
    "\tscores = None\n",
    "\ttry:\n",
    "\t\twith warnings.catch_warnings():\n",
    "\t\t\twarnings.filterwarnings(\"ignore\")\n",
    "\t\t\tscores = evaluate_model(X, y, model, folds, metric, pipe_func)\n",
    "\texcept:\n",
    "\t\tscores = None\n",
    "\treturn scores\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(X, y, models, pipe_funcs, folds=10, metric='recall'):\n",
    "\tresults = dict()\n",
    "\tfor name, model in models.items():\n",
    "\t\t# evaluate model under each preparation function\n",
    "\t\tfor i in range(len(pipe_funcs)):\n",
    "\t\t\t# evaluate the model\n",
    "\t\t\tscores = robust_evaluate_model(X, y, model, folds, metric, pipe_funcs[i])\n",
    "\t\t\t# update name\n",
    "\t\t\trun_name = str(i) + name\n",
    "\t\t\t# show process\n",
    "\t\t\tif scores is not None:\n",
    "\t\t\t\t# store a result\n",
    "\t\t\t\tresults[run_name] = scores\n",
    "\t\t\t\tmean_score, std_score = mean(scores), std(scores)\n",
    "\t\t\t\tprint('>%s: %.3f (+/-%.3f)' % (run_name, mean_score, std_score))\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint('>%s: error' % run_name)\n",
    "\treturn results\n",
    "\n",
    "# print and plot the top n results\n",
    "def summarize_results(results, maximize=True, top_n=10):\n",
    "\t# check for no results\n",
    "\tif len(results) == 0:\n",
    "\t\tprint('no results')\n",
    "\t\treturn\n",
    "\t# determine how many results to summarize\n",
    "\tn = min(top_n, len(results))\n",
    "\t# create a list of (name, mean(scores)) tuples\n",
    "\tmean_scores = [(k,mean(v)) for k,v in results.items()]\n",
    "\t# sort tuples by mean score\n",
    "\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "\t# reverse for descending order (e.g. for accuracy)\n",
    "\tif maximize:\n",
    "\t\tmean_scores = list(reversed(mean_scores))\n",
    "\t# retrieve the top n for summarization\n",
    "\tnames = [x[0] for x in mean_scores[:n]]\n",
    "\tscores = [results[x[0]] for x in mean_scores[:n]]\n",
    "\t# print the top n\n",
    "\tprint()\n",
    "\tfor i in range(n):\n",
    "\t\tname = names[i]\n",
    "\t\tmean_score, std_score = mean(results[name]), std(results[name])\n",
    "\t\tprint('Rank=%d, Name=%s, Score=%.3f (+/- %.3f)' % (i+1, name, mean_score, std_score))\n",
    "\t# boxplot for the top n\n",
    "\tpyplot.boxplot(scores, labels=names)\n",
    "\t_, labels = pyplot.xticks()\n",
    "\tpyplot.setp(labels, rotation=90)\n",
    "\tpyplot.savefig('spotcheck.png')\n",
    "\n",
    "# load dataset\n",
    "X, y = load_dataset()\n",
    "# get model list\n",
    "models = define_models()\n",
    "# define transform pipelines\n",
    "pipelines = [pipeline_none, pipeline_standardize, pipeline_normalize, pipeline_std_norm]\n",
    "# evaluate models\n",
    "results = evaluate_models(X, y, models, pipelines)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of grid searching key hyperparameters for GradientBoostingClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# define dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "# define models and parameters\n",
    "model = GradientBoostingClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.7, 1.0]\n",
    "max_depth = [3, 7, 9]\n",
    "# define grid search\n",
    "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-collective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-invalid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-figure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "numeric-alcohol",
   "metadata": {},
   "source": [
    "# Randomized Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running RandomizedSearchCV')\n",
    "# default RFClassifier but set random_state = 0 to keep consistent results\n",
    "MOD = RandomForestClassifier(random_state=0) \n",
    "#Implement RandomSearchCV\n",
    "# Number of trees in random forest [100,150,...,500]\n",
    "n_estimators = [int(x) for x in np.arange(start = 100, stop = 501, step = 50)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.arange(start = 20, stop = 101, step = 20)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "scoreFunction = {\"recall\": \"recall\"}\n",
    "# run a RandomizedSearchCV with 3 folds and 25 iterations \n",
    "random_search = RandomizedSearchCV(MOD,\n",
    "                                   param_distributions = random_grid,\n",
    "                                   n_iter = 25,\n",
    "                                   scoring = scoreFunction,               \n",
    "                                   refit = \"recall\",\n",
    "                                   return_train_score = False,\n",
    "                                   random_state = 0,\n",
    "                                   verbose = 2,\n",
    "                                   cv = 3,\n",
    "                                   n_jobs = -1) \n",
    "\n",
    "#trains and optimizes the model\n",
    "random_search.fit(X_train80, y_train80)\n",
    "\n",
    "print('Finished RandomizedSearchCV ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hollywood-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-entrepreneur",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "understood-burton",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the dataset, returns X and y elements\n",
    "def load_dataset():\n",
    "    d = pd.read_csv('source/d_num.csv')\n",
    "    d_values = d.values\n",
    "    x, y = d_values[:,1:], d_values[:,:1].ravel()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-conversion",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "south-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=list()):\n",
    "    # linear\n",
    "    i = 1000\n",
    "    models.append(('LR', LogisticRegression(solver='saga', max_iter=i, class_weight='balanced', random_state=5))) # note: `max_iter` from 1000 to 10000 due to convergence issues\n",
    "    # non-linear\n",
    "    models.append(('DT', DecisionTreeClassifier(random_state=5)))\n",
    "    i = 1000\n",
    "    models.append(('MLP_' + str(i), MLPClassifier(max_iter=i, random_state=5)))\n",
    "    # ensemble\n",
    "    t = 800\n",
    "    models.append(('ADA_' + str(t), AdaBoostClassifier(n_estimators=t, random_state=5))) \n",
    "    models.append(('GB_' + str(t), GradientBoostingClassifier(n_estimators=t, random_state=5))) # note: `max_iter` from 100 to 1000 due to convergence issues\n",
    "    print(f'Defined {len(models)} models.')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-devil",
   "metadata": {},
   "source": [
    "## Define Resamplers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-withdrawal",
   "metadata": {},
   "source": [
    "[Source: How to Combine Oversampling and Undersampling for Imbalanced Classification, *Machine Learning Mastery*](https://machinelearningmastery.com/combine-oversampling-and-undersampling-for-imbalanced-classification/)\n",
    "<br>[Source: Undersampling Algorithms for Imbalanced Classification, *Machine Learning Mastery*](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "demonstrated-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_resamplers(resamplers=list()):\n",
    "    resamplers.append(('SM', SMOTE(random_state=5, n_jobs=-1)))\n",
    "    resamplers.append(('SM_TM', SMOTETomek(tomek=TomekLinks(sampling_strategy='majority')))) \n",
    "    resamplers.append(('SM_ENN', SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority')))) \n",
    "    print(f'Defined {len(resamplers)} resamplers.')\n",
    "    return resamplers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-amendment",
   "metadata": {},
   "source": [
    "## Build Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "positive-shopping",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize and standardize transform pipeline : RS + Resampling\n",
    "def pipeline_RS_SS_resample(model, resampler):\n",
    "    steps = list()\n",
    "    # normalization\n",
    "    steps.append(('RS', RobustScaler()))\n",
    "    # standardization\n",
    "    steps.append(('SS', StandardScaler()))\n",
    "    # the resampler\n",
    "    steps.append(('RSP', resampler))\n",
    "    # the model\n",
    "    steps.append(('MOD', model))\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-pipeline",
   "metadata": {},
   "source": [
    "## Try One Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\n",
    "from sklearn.metrics import make_scorer,fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sunset-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset()\n",
    "models = define_models()\n",
    "resamplers = define_resamplers()\n",
    "for i in resamplers:\n",
    "    print(i)\n",
    "for i in models:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "funny-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models[4][1])\n",
    "print(resamplers[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "careful-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[4][1]\n",
    "resampler = resamplers[2][1]\n",
    "pipeline = pipeline_RS_SS_resample(model, resampler)\n",
    "scores = cross_val_score(pipeline, X, y, scoring='recall', cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "reverse-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cv method\n",
    "n_splits = 10\n",
    "n_repeats = 5\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=1)  \n",
    "\n",
    "# define performance metrics\n",
    "scoring = {\n",
    "    'accuracy':'accuracy', 'precision':'precision', 'recall':'recall', 'f1':'f1', \n",
    "    'f2':make_scorer(fbeta_score, beta=2)} # dict val = scorer fct or predefined metric str  \n",
    "\n",
    "# evaluate result\n",
    "result = cross_validate(\n",
    "    pipeline, X, y, cv=cv, \n",
    "    scoring=scoring, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "# make a summary table\n",
    "df = pd.DataFrame(\n",
    "    (k, np.nanmean(v), np.nanstd(v)) for k,v in result.items()\n",
    "    ).rename({0:'metric', 1:'mean', 2:'std'}, axis=1\n",
    "            ).set_index('metric')\n",
    "df.index.name = None\n",
    "df.columns = pd.MultiIndex.from_product([['mod_disp_name'],df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "owned-religious",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">mod_disp_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>201.53</td>\n",
       "      <td>24.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_accuracy</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_precision</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_precision</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_recall</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_recall</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f1</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f2</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_f2</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mod_disp_name      \n",
       "                         mean   std\n",
       "fit_time               201.53 24.98\n",
       "score_time               0.08  0.04\n",
       "test_accuracy            0.97  0.00\n",
       "train_accuracy           1.00  0.00\n",
       "test_precision           0.93  0.02\n",
       "train_precision          1.00  0.00\n",
       "test_recall              0.91  0.02\n",
       "train_recall             0.99  0.00\n",
       "test_f1                  0.92  0.01\n",
       "train_f1                 0.99  0.00\n",
       "test_f2                  0.91  0.02\n",
       "train_f2                 0.99  0.00"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
