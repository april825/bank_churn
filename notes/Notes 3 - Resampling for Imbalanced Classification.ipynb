{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "juvenile-neighborhood",
   "metadata": {},
   "source": [
    "# Resampling for Imbalanced Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-buffer",
   "metadata": {},
   "source": [
    "[Machine Learning Mastery](https://machinelearningmastery.com/combine-oversampling-and-undersampling-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-swing",
   "metadata": {},
   "source": [
    "# Manually Combine Random Oversampling and Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-advocate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combination of random oversampling and undersampling for imbalanced classification\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "# define model\n",
    "model = DecisionTreeClassifier()\n",
    "# define resampling\n",
    "over = RandomOverSampler(sampling_strategy=0.1)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "# define pipeline\n",
    "pipeline = Pipeline(steps=[('o', over), ('u', under), ('m', model)])\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-gross",
   "metadata": {},
   "source": [
    "# Manually Combine SMOTE and Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combination of SMOTE and random undersampling for imbalanced classification\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "# define model\n",
    "model = DecisionTreeClassifier()\n",
    "# define pipeline\n",
    "over = SMOTE(sampling_strategy=0.1)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "steps = [('o', over), ('u', under), ('m', model)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-guidance",
   "metadata": {},
   "source": [
    "# Use Predefined Combinations of Resampling Methods\n",
    "There are combinations of oversampling and undersampling methods that have proven effective and together may be considered resampling techniques. Two examples are the combination of SMOTE with Tomek Links undersampling and SMOTE with Edited Nearest Neighbors undersampling. The imbalanced-learn Python library provides implementations for both of these combinations directly. Let’s take a closer look at each in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-database",
   "metadata": {},
   "source": [
    "## Combination of SMOTE and Tomek Links Undersampling\n",
    "SMOTE is an oversampling method that synthesizes new plausible examples in the majority class.\n",
    "\n",
    "Tomek Links refers to a method for identifying pairs of nearest neighbors in a dataset that have different classes. Removing one or both of the examples in these pairs (such as the examples in the majority class) has the effect of making the decision boundary in the training dataset less noisy or ambiguous. Gustavo Batista, et al. tested combining these methods in their 2003 paper titled \"Balancing Training Data for Automated Annotation of Keywords: a Case Study.\" Specifically, first the SMOTE method is applied to oversample the minority class to a balanced distribution, then examples in Tomek Links from the majority classes are identified and removed. \n",
    "\n",
    "> In this work, only majority class examples that participate of a Tomek link were removed, since minority class examples were considered too rare to be discarded. […] In our work, as minority class examples were artificially created and the data sets are currently balanced, then both majority and minority class examples that form a Tomek link, are removed. \n",
    "\n",
    "*— Balancing Training Data for Automated Annotation of Keywords: a Case Study, 2003.*\n",
    "\n",
    "The combination was shown to provide a reduction in false negatives at the cost of an increase in false positives for a binary classification task. We can implement this combination using the SMOTETomek class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined SMOTE and Tomek Links resampling for imbalanced classification\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "# define model\n",
    "model = DecisionTreeClassifier()\n",
    "# define resampling\n",
    "resample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
    "# define pipeline\n",
    "pipeline = Pipeline(steps=[('r', resample), ('m', model)])\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-october",
   "metadata": {},
   "source": [
    "## Combination of SMOTE and Edited Nearest Neighbors Undersampling\n",
    "SMOTE may be the most popular oversampling technique and can be combined with many different undersampling techniques.\n",
    "\n",
    "Another very popular undersampling method is the Edited Nearest Neighbors, or ENN, rule. This rule involves using k=3 nearest neighbors to locate those examples in a dataset that are misclassified and that are then removed. It can be applied to all classes or just those examples in the majority class. Gustavo Batista, et al. explore many combinations of oversampling and undersampling methods compared to the methods used in isolation in their 2004 paper titled \"A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data.\"\n",
    "\n",
    "This includes the combinations:\n",
    "\n",
    "* Condensed Nearest Neighbors + Tomek Links\n",
    "* SMOTE + Tomek Links\n",
    "* SMOTE + Edited NearestNeighbors\n",
    "\n",
    "Regarding this final combination, the authors comment that ***ENN is more aggressive at downsampling the majority class than Tomek Links***, providing more in-depth cleaning. They apply the method, removing examples from both the majority and minority classes.\n",
    "\n",
    "> … ENN is used to remove examples from both classes. Thus, any example that is misclassified by its three nearest neighbors is removed from the training set.\n",
    "\n",
    "*— A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data, 2004.*\n",
    "\n",
    "This can be implemented via the `SMOTEENN` class in the imbalanced-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined SMOTE and Edited Nearest Neighbors resampling for imbalanced classification\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.combine import SMOTEENN\n",
    "# generate dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n",
    "\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n",
    "# define model\n",
    "model = DecisionTreeClassifier()\n",
    "# define resampling\n",
    "resample = SMOTEENN()\n",
    "# define pipeline\n",
    "pipeline = Pipeline(steps=[('r', resample), ('m', model)])\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-intellectual",
   "metadata": {},
   "source": [
    "Source: Stack Overflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-sierra",
   "metadata": {},
   "source": [
    "# Making a Manual Class for Outlier Treatment for the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, TransformerMixin\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "class OutlierExtractor(TransformerMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Create a transformer to remove outliers. A threshold is set for selection\n",
    "        criteria, and further arguments are passed to the LocalOutlierFactor class\n",
    "\n",
    "        Keyword Args:\n",
    "            neg_conf_val (float): The threshold for excluding samples with a lower\n",
    "               negative outlier factor.\n",
    "\n",
    "        Returns:\n",
    "            object: to be used as a transformer method as part of Pipeline()\n",
    "        \"\"\"\n",
    "\n",
    "        self.threshold = kwargs.pop('neg_conf_val', -10.0)\n",
    "\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def transform(self, X, y):\n",
    "        \"\"\"\n",
    "        Uses LocalOutlierFactor class to subselect data based on some threshold\n",
    "\n",
    "        Returns:\n",
    "            ndarray: subsampled data\n",
    "\n",
    "        Notes:\n",
    "            X should be of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        lcf = LocalOutlierFactor(**self.kwargs)\n",
    "        lcf.fit(X)\n",
    "        return (X[lcf.negative_outlier_factor_ > self.threshold, :],\n",
    "                y[lcf.negative_outlier_factor_ > self.threshold])\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([('outliers', OutlierExtraction()), ...])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
