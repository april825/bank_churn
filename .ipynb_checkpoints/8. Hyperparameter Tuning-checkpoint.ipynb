{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expensive-client",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:rgb(67, 77, 86);\n",
    "           font-size:300%;\n",
    "           font-style: oblique;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 20px;\">Predicting Bank Churners</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-individual",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "<h2 style=\"background-color:rgb(141, 153, 165);\n",
    "           font-size:250%;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 10px;\">Chapter 5. Spot Check Version 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-expense",
   "metadata": {},
   "source": [
    "<a id='1.1'>\n",
    "    <h2 style='font-size:180%;'>\n",
    "        Mission</h2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-candle",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <blockquote cite='https://www.kaggle.com/sakshigoyal7/credit-card-customers/tasks?taskId=2729'>\n",
    "        <p style='font-size:110%;\n",
    "                  color:hsl(208, 12%, 30%);'><i>Our top priority in this business problem is to identify customers who are getting churned. Even if we predict non-churning customers as churned, it won't harm our business. But predicting churning customers as non-churning will do. So recall needs to be higher. Till now, I have managed to get a recall of 62%.</i></p>\n",
    "    </blockquote>\n",
    "    <figcaption>â€”Sakshi Goyal, <cite>Credit Card Customers, Kaggle</cite></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-height",
   "metadata": {},
   "source": [
    "<a id='4.1'>\n",
    "    <h2 style='font-size:180%;'>\n",
    "        Libraries</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inclusive-pattern",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# binary classification spot check script\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "arranged-power",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sustained-medium",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "/* CSS styles for pandas dataframe */\n",
       ".dataframe th {\n",
       "    font-size: 16px;\n",
       "}\n",
       ".dataframe td {\n",
       "    font-size: 14px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* CSS styles for pandas dataframe */\n",
    ".dataframe th {\n",
    "    font-size: 16px;\n",
    "}\n",
    ".dataframe td {\n",
    "    font-size: 14px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mathematical-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-pantyhose",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "<h2 style=\"background-color:rgb(141, 153, 165);\n",
    "           font-size:250%;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 10px;\">Spot Check for Model & Scaler</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-victorian",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-graphic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the dataset, returns X and y elements\n",
    "def load_dataset():\n",
    "    d = pd.read_csv('source/d_num.csv')\n",
    "    d_values = d.values\n",
    "    x, y = d_values[:,1:], d_values[:,:1].ravel()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-density",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=list()):\n",
    "    \n",
    "    # linear\n",
    "    i = 1000\n",
    "    models.append(('LR', LogisticRegression(solver='saga', max_iter=i, class_weight='balanced', random_state=5))) # note: `max_iter` from 1000 to 10000 due to convergence issues\n",
    "   \n",
    "    t = 800\n",
    "    models.append(('ADA_' + str(t), AdaBoostClassifier(n_estimators=t, random_state=5))) \n",
    "    models.append(('GB_' + str(t), GradientBoostingClassifier(n_estimators=t, random_state=5))) # note: `max_iter` from 100 to 1000 due to convergence issues\n",
    "    \n",
    "    print(f'Defined {len(models)} models.')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-dependence",
   "metadata": {},
   "source": [
    "## Define Resamplers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-fence",
   "metadata": {},
   "source": [
    "[Source: How to Combine Oversampling and Undersampling for Imbalanced Classification, *Machine Learning Mastery*](https://machinelearningmastery.com/combine-oversampling-and-undersampling-for-imbalanced-classification/)\n",
    "<br>[Source: Undersampling Algorithms for Imbalanced Classification, *Machine Learning Mastery*](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_resamplers(resamplers=list()):\n",
    "    resamplers.append(('SM_ENN', SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority')))) \n",
    "    print(f'Defined {len(resamplers)} resamplers.')\n",
    "    return resamplers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-hacker",
   "metadata": {},
   "source": [
    "## Build Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-biodiversity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize and standardize transform pipeline : QT + Resampling\n",
    "def pipeline_QTSS_ENN(model, resampler):\n",
    "    steps = list()\n",
    "    # normalization\n",
    "    steps.append(('QT', QuantileTransformer()))\n",
    "    # standardization\n",
    "    steps.append(('SS', StandardScaler()))\n",
    "    # the resampler\n",
    "    steps.append(('RSP', resampler))\n",
    "    # the model\n",
    "    steps.append(('MOD', model))\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline\n",
    "\n",
    "# normalize and standardize transform pipeline : RS + Resampling\n",
    "def pipeline_RSSS_ENN(model, resampler):\n",
    "    steps = list()\n",
    "    # normalization\n",
    "    steps.append(('RS', RobustScaler()))\n",
    "    # standardization\n",
    "    steps.append(('SS', StandardScaler()))\n",
    "    # the resampler\n",
    "    steps.append(('RSP', resampler))\n",
    "    # the model\n",
    "    steps.append(('MOD', model))\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-session",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-clear",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-powell",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perf_metrics(y_test, y_pred):\n",
    "    dic = {}\n",
    "    dic['accuracy'] = round(accuracy_score(y_test, y_pred), 2)\n",
    "    dic['precision'] = round(precision_score(y_test, y_pred), 2)\n",
    "    dic['recall'] = round(recall_score(y_test, y_pred), 2)\n",
    "    dic['f1'] = round(f1_score(y_test, y_pred), 2)\n",
    "    dic['f2'] = round(fbeta_score(y_test, y_pred, beta=2), 2)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-bachelor",
   "metadata": {},
   "source": [
    "### Result Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-exchange",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def result_rskf(x, y, pipeline, mod_disp_name, n_splits=5, n_repeats=3):\n",
    "   \n",
    "    # define cv method\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits, n_repeats=n_repeats, random_state=1)  \n",
    "    \n",
    "    # define performance metrics\n",
    "    scoring = {\n",
    "        'accuracy':'accuracy', 'precision':'precision', 'recall':'recall', 'f1':'f1', \n",
    "        'f2':make_scorer(fbeta_score, beta=2)} # dict val = scorer fct or predefined metric str  \n",
    "    \n",
    "    # evaluate result\n",
    "    result = cross_validate(\n",
    "        pipeline, x, y, cv=cv, \n",
    "        scoring=scoring, return_train_score=True, n_jobs=-1)\n",
    "        \n",
    "    # make a summary table\n",
    "    df = pd.DataFrame(\n",
    "        (k, mean(v), std(v)) for k,v in result.items()\n",
    "        ).rename({0:'metric', 1:'mean', 2:'std'}, axis=1\n",
    "                ).set_index('metric')\n",
    "    df.index.name = None\n",
    "    df.columns = pd.MultiIndex.from_product([[mod_disp_name],df.columns])\n",
    "    \n",
    "    return df, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-unemployment",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def result_tts(x, y, pipeline, mod_disp_name):\n",
    "    \n",
    "    # define cv method\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.2, random_state=1, shuffle=True, stratify=y)\n",
    "    \n",
    "    # evaluate result   \n",
    "    time_0 = time.time()\n",
    "    pipeline.fit(x_train, y_train)\n",
    "    time_1 = time.time()\n",
    "    y_pred = pipeline.predict(x_test)\n",
    "    time_2 = time.time()\n",
    "    result = {}\n",
    "    result['fit_time'] = round(time_1-time_0, 2)\n",
    "    result['score_time'] = round(time_2-time_1, 2)\n",
    "    result['accuracy'] = round(accuracy_score(y_test, y_pred), 2)\n",
    "    result['precision'] = round(precision_score(y_test, y_pred), 2)\n",
    "    result['recall'] = round(recall_score(y_test, y_pred), 2)\n",
    "    result['f1'] = round(f1_score(y_test, y_pred), 2)\n",
    "    result['f2'] = round(fbeta_score(y_test, y_pred, beta=2), 2)\n",
    "    conf_mat = confusion_matrix(y_test, y_pred, labels=[1,0])\n",
    "    \n",
    "    # make a summary table    \n",
    "    df = pd.DataFrame(result, index=[mod_disp_name]).T\n",
    "    \n",
    "    return df, result, conf_mat, y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-cradle",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summary_by_mod(x, y, models, scalers, result_func=result_rskf, **n_splits_and_repeats):\n",
    "    results = []\n",
    "    time_0 = time.time() # for all methods in pipeline\n",
    "    for scaler in scalers:\n",
    "        results_models = []\n",
    "        time_1 = time.time() # for each scaler\n",
    "        print(f'Scaler: {scaler[0]}\\n')\n",
    "        for model in models:\n",
    "            time_2 = time.time() # for each model\n",
    "            pipeline = Pipeline([('s', scaler[1]), ('m', model[1])])\n",
    "            if result_func==result_rskf:\n",
    "                n_splits, n_repeats = (i for i in n_splits_and_repeats.values())\n",
    "                results_model = result_func(x, y, pipeline, model[0], n_splits, n_repeats)[0]\n",
    "            else:\n",
    "                results_model = result_func(x, y, pipeline, model[0])[0]\n",
    "            print(f'Model {model[0]} Runtime: {time.strftime(\"%M:%S\", time.gmtime(time.time()-time_2))}')\n",
    "            results_models.append(results_model)\n",
    "        print(f'Scaler {scaler[0]} Avg Runtime per Model: {time.strftime(\"%M:%S\", time.gmtime((time.time()-time_1)/len(models)))}\\n\\n')\n",
    "        results.append(results_models)\n",
    "    print(f'Total Runtime: {time.strftime(\"%M:%S\", time.gmtime(time.time()-time_0))} min')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-public",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-hotel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(X, y, model, resampler, folds, metric, pipe_func):\n",
    "    \n",
    "    # create the pipeline\n",
    "    pipeline = pipe_func(model, resampler)\n",
    "    \n",
    "    # define cv method\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=10, n_repeats=5, random_state=5)  \n",
    "    \n",
    "    # define performance metrics\n",
    "    scoring = {\n",
    "        'accuracy':'accuracy', 'precision':'precision', 'recall':'recall', 'f1':'f1', \n",
    "        'f2':make_scorer(fbeta_score, beta=2)} # dict val = scorer fct or predefined metric str  \n",
    "    \n",
    "    # evaluate result\n",
    "    scores = cross_validate(\n",
    "        pipeline, x, y, cv=cv, \n",
    "        scoring=scoring, return_train_score=True, n_jobs=-1)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "# evaluate a model and try to trap errors and and hide warnings\n",
    "def robust_evaluate_model(X, y, model, resampler, folds, metric, pipe_func):\n",
    "    scores = None\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            scores = evaluate_model(X, y, model, resampler, folds, metric, pipe_func)\n",
    "    except:\n",
    "        scores = None\n",
    "    return scores\n",
    "\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(X, y, models, resamplers, pipe_funcs, folds=10, metric='recall'):\n",
    "    results = dict()\n",
    "    for name_model, model in models:\n",
    "        for name_resampler, resampler in resamplers:\n",
    "            # evaluate model under each preparation function\n",
    "            for i in range(len(pipe_funcs)):\n",
    "                # evaluate the model\n",
    "                scores = robust_evaluate_model(X, y, model, resampler, folds, metric, pipe_funcs[i])\n",
    "                # update name\n",
    "                run_name = str(i) + '_' + name_model + '_' + name_resampler\n",
    "                # show process\n",
    "                if scores is not None:\n",
    "                    # store a result\n",
    "                    results[run_name] = scores\n",
    "                    mean_score, std_score = np.nanmean(scores), np.nanstd(scores)\n",
    "                    print('>%s: %.3f (+/-%.3f)' % (run_name, mean_score, std_score))\n",
    "                else:\n",
    "                    print('>%s: error' % run_name)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-nudist",
   "metadata": {},
   "source": [
    "## Examine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-jamaica",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print and plot the top n results\n",
    "def summarize_results(results, maximize=True, top_n=10):\n",
    "\t# check for no results\n",
    "\tif len(results) == 0:\n",
    "\t\tprint('no results')\n",
    "\t\treturn\n",
    "\t# determine how many results to summarize\n",
    "\tn = min(top_n, len(results))\n",
    "\t# create a list of (name, mean(scores)) tuples\n",
    "\tmean_scores = [(k, np.nanmean(v)) for k,v in results.items()]\n",
    "\t# sort tuples by mean score\n",
    "\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "\t# reverse for descending order (e.g. for accuracy)\n",
    "\tif maximize:\n",
    "\t\tmean_scores = list(reversed(mean_scores))\n",
    "\t# retrieve the top n for summarization\n",
    "\tnames = [x[0] for x in mean_scores[:n]]\n",
    "\tscores = [results[x[0]] for x in mean_scores[:n]]\n",
    "\t# print the top n\n",
    "\tprint()\n",
    "\tfor i in range(n):\n",
    "\t\tname = names[i]\n",
    "\t\tmean_score, std_score = np.nanmean(results[name]), np.nanstd(results[name])\n",
    "\t\tprint('Rank=%d, Name=%s, Score=%.3f (+/- %.3f)' % (i+1, name, mean_score, std_score))\n",
    "\t# boxplot for the top n\n",
    "\tpyplot.boxplot(scores, labels=names)\n",
    "\t_, labels = pyplot.xticks()\n",
    "\tpyplot.setp(labels, rotation=90)\n",
    "\tpyplot.savefig('spotcheck.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-procurement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-camping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-firmware",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fatty-canyon",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the dataset, returns X and y elements\n",
    "def load_dataset():\n",
    "    d = pd.read_csv('source/d_num.csv')\n",
    "    d_values = d.values\n",
    "    x, y = d_values[:,1:], d_values[:,:1].ravel()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "welsh-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "logit = LogisticRegression(random_state=5)\n",
    "ada = AdaBoostClassifier(random_state=5)\n",
    "gb = GradientBoostingClassifier(random_state=5)\n",
    "\n",
    "# scalers\n",
    "rs = ('RS', RobustScaler())\n",
    "qs = ('QT', QuantileTransformer())\n",
    "\n",
    "def pipeline_scaler_ENN(scaler, model):\n",
    "    steps = list()\n",
    "    # normalization\n",
    "    steps.append((scaler[0], scaler[1]))\n",
    "    # standardization\n",
    "    steps.append(('SS', StandardScaler()))\n",
    "    # the resampler\n",
    "    steps.append(('RSP', SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))))\n",
    "    # the model\n",
    "    steps.append(('MOD', model))\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cognitive-attention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('QT', QuantileTransformer()), ('SS', StandardScaler()),\n",
       "                ('RSP',\n",
       "                 SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))),\n",
       "                ('MOD', GradientBoostingClassifier(random_state=5))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_scaler_ENN(qs, gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of grid searching key hyperparameters for GradientBoostingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# define dataset\n",
    "X, y = load_dataset()\n",
    "# define models and parameters\n",
    "model = pipeline_scaler_ENN(qs, gb)\n",
    "n_estimators = [100, 300, 800]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.7, 1.0]\n",
    "max_depth = [3, 7, 9]\n",
    "# define grid search\n",
    "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='recall', error_score=0)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-particular",
   "metadata": {},
   "source": [
    "## Run Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-indicator",
   "metadata": {},
   "source": [
    "Across all models, the SMOTE oversampling method in conjunction with the Edited Nearest Neighbor undersampling performed the best. Among the predictive algorithms, GB, ADA, and LR look promising. In the next session, we will narrow down our models and jump into hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-gauge",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X, y = load_dataset()\n",
    "# get model list\n",
    "models = define_models()\n",
    "# get model list\n",
    "resamplers = define_resamplers()\n",
    "# define transform pipelines\n",
    "pipelines = [pipeline_QTSS_ENN, pipeline_RSSS_ENN]\n",
    "# evaluate models\n",
    "results = evaluate_models(X, y, models, resamplers, pipelines)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-oxygen",
   "metadata": {},
   "source": [
    "## Calculate Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_1 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished in {round(time_1-time_0, 2):,} second(s) or {round((time_1-time_0)/60, 2)} minute(s).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank=1, Name=2_GB_800_SM_ENN, Score=0.914 (+/- 0.063)\n",
    "# Rank=2, Name=2_ADA_800_SM_ENN, Score=0.910 (+/- 0.056)\n",
    "# Rank=3, Name=1_GB_800_SM_ENN, Score=0.908 (+/- 0.087)\n",
    "# Rank=4, Name=1_ADA_800_SM_ENN, Score=0.906 (+/- 0.063)\n",
    "# Rank=5, Name=1_LR_SM_ENN, Score=0.870 (+/- 0.107)\n",
    "# Rank=6, Name=2_LR_SM_ENN, Score=0.868 (+/- 0.075)\n",
    "# Rank=7, Name=2_MLP_1000_SM_ENN, Score=0.867 (+/- 0.078)\n",
    "# Rank=8, Name=1_GB_800_SM_TM, Score=0.859 (+/- 0.099)\n",
    "# Rank=9, Name=2_GB_800_SM, Score=0.857 (+/- 0.099)\n",
    "# Rank=10, Name=1_GB_800_SM, Score=0.857 (+/- 0.096)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
