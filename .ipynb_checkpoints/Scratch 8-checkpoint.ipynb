{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-shark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "gothic-concentration",
   "metadata": {},
   "source": [
    "## MODIFIED 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "polar-alignment",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def result_rskf1(d, pipeline, mod_disp_name, n_splits=5, n_repeats=3):\n",
    "   \n",
    "    # feature/row selection\n",
    "    d_values = d.values\n",
    "    x, y = d_values[:,1:], d_values[:,:1].ravel()\n",
    "    \n",
    "    # define cv method\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits, n_repeats=n_repeats, random_state=1)  \n",
    "    \n",
    "    # define performance metrics\n",
    "    scoring = {\n",
    "        'accuracy':'accuracy', 'precision':'precision', 'recall':'recall', 'f1':'f1', \n",
    "        'f2':make_scorer(fbeta_score, beta=2)} # dict val = scorer fct or predefined metric str  \n",
    "    \n",
    "    # evaluate result\n",
    "    result = cross_validate(\n",
    "        pipeline, x, y, cv=cv, \n",
    "        scoring=scoring, return_train_score=True, n_jobs=-1)\n",
    "        \n",
    "    # make a summary table\n",
    "    df = pd.DataFrame(\n",
    "        (k, mean(v), std(v)) for k,v in result.items()\n",
    "        ).rename({0:'metric', 1:'mean', 2:'std'}, axis=1\n",
    "                ).set_index('metric')\n",
    "    df.index.name = None\n",
    "    df.columns = pd.MultiIndex.from_product([[mod_disp_name],df.columns])\n",
    "    \n",
    "    return df, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-davis",
   "metadata": {},
   "source": [
    "## MODIFIED 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "imperial-membrane",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def result_rskf2(d, pipeline, mod_disp_name, n_splits=5, n_repeats=3):\n",
    "    \n",
    "    # feature selection\n",
    "    d_values = d.values\n",
    "    x, y = d_values[:,1:], d_values[:,:1].ravel()\n",
    "\n",
    "    # define cv method\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits, n_repeats=n_repeats, random_state=1)  \n",
    "    \n",
    "    # define performance metrics\n",
    "    scoring = {\n",
    "        'accuracy':'accuracy', 'precision':'precision', 'recall':'recall', 'f1':'f1', \n",
    "        'f2':make_scorer(fbeta_score, beta=2)} # dict val = scorer fct or predefined metric str  \n",
    "    \n",
    "    # evaluate result\n",
    "    result = cross_validate(\n",
    "        pipeline, x, y, cv=cv, \n",
    "        scoring=scoring, return_train_score=True, n_jobs=-1)\n",
    "        \n",
    "    # make a summary table\n",
    "    df = pd.DataFrame(\n",
    "        (k, mean(v), std(v)) for k,v in result.items()\n",
    "        ).rename({0:'metric', 1:'mean', 2:'std'}, axis=1\n",
    "                ).set_index('metric')\n",
    "    df.index.name = None\n",
    "    df.columns = pd.MultiIndex.from_product([[mod_disp_name],df.columns])\n",
    "    \n",
    "    return df, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-athens",
   "metadata": {},
   "source": [
    "# MODIFIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "occupied-position",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summary_by_mod2(pred_mods, scalers, features, **n_splits_and_repeats):\n",
    "    \n",
    "    time_0 = time.time() # for all methods in pipeline\n",
    "    \n",
    "    results = []\n",
    "    for scaler in scalers:\n",
    "        time_1 = time.time() # for each scaler\n",
    "        print(f'Scaler: {scaler[0]}\\n')\n",
    "        \n",
    "        results_features = []\n",
    "        for feature in features:\n",
    "            time_2 = time.time() # for each feature selection model\n",
    "            print(f'FS Model: {feature[0]}\\n')\n",
    "            \n",
    "            results_pred_mods = []\n",
    "            for pred_mod in pred_mods:\n",
    "                time_3 = time.time() # for each prediction model\n",
    "                print(f'Prediction Model: {pred_mod[0]}\\n')\n",
    "                \n",
    "                # define pipeline\n",
    "                pipeline = Pipeline([('s', scaler[1]), ('fs', feature[1]), ('m', pred_mod[1])])\n",
    "\n",
    "                # fit models\n",
    "                n_splits, n_repeats = (i for i in n_splits_and_repeats.values())\n",
    "                results_model = result_rskf(x, y, pipeline, pred_mod[0], n_splits, n_repeats)[0]\n",
    "                \n",
    "                # collect results - innermost\n",
    "                print(f'Model {pred_mod[0]} Runtime: {time.strftime(\"%M:%S\", time.gmtime(time.time()-time_3))}')\n",
    "                results_pred_mods.append(results_model)\n",
    "            \n",
    "            # collect results - middle\n",
    "            print(f'FS Model {feature[0]} Avg Runtime per Model: {time.strftime(\"%M:%S\", time.gmtime((time.time()-time_2)/len(pred_mods)))}\\n\\n')\n",
    "            results_features.append(results_pred_mods)\n",
    "        \n",
    "        # collect results - outermost\n",
    "        print(f'Scaler {scaler[0]} Avg Runtime per Model: {time.strftime(\"%M:%S\", time.gmtime((time.time()-time_1)/len(features)))}\\n\\n')\n",
    "        results.append(results_features)\n",
    "        \n",
    "    print(f'Total Runtime: {time.strftime(\"%M:%S\", time.gmtime(time.time()-time_0))} min')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-colon",
   "metadata": {},
   "source": [
    "# ANOVA F-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ANOVA F-value for the provided sample.\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, p = f_classif(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-section",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "pressed-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = {}\n",
    "for i in np.arange(x_train.shape[1]):\n",
    "    ref[i]=d.iloc[:,1:].columns[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ANOVA F-value for the provided sample.\n",
    "from sklearn.feature_selection import f_classif\n",
    "f, p = f_classif(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_ftest = pd.DataFrame(p, columns=['p-val'])\n",
    "pval_ftest.index = d.iloc[:,1:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pval_ftest.style.set_table_attributes('style=\"font-size: 15px\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_ftest[pval_ftest['p-val']<.16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_indices = np.arange(x_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Univariate feature selection with F-test for feature scoring\n",
    "# We use the default selection function to select the four\n",
    "# most significant features\n",
    "\n",
    "selector = SelectKBest(f_classif, k=7)\n",
    "selector.fit(x_train, y_train)\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "scores /= scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.bar(X_indices - .45, scores, width=.8,\n",
    "        label=r'Univariate score ($-Log(p_{value})$)')\n",
    "plt.xlabel('Features')\n",
    "plt.xticks(X_indices, d.iloc[:,1:].columns, rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA feature selection for numeric input and categorical output\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# define feature selection\n",
    "fs = SelectKBest(score_func=f_classif, k=8)\n",
    "# apply feature selection\n",
    "x_selected = fs.fit_transform(x_train, y_train)\n",
    "print(x_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-folder",
   "metadata": {},
   "source": [
    "For categorical predictors, we will use the chi-squared test and mutual information (information gain) from the field of information theory. Mutual information is agnostic to the data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-being",
   "metadata": {},
   "source": [
    "# Filtered Data Anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_values = d[var_to_keep].values\n",
    "x, y = d_values[:,1:], d_values[:,:1].ravel()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = {}\n",
    "for i in np.arange(x_train.shape[1]):\n",
    "    ref[i]=d.iloc[:,1:].columns[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ANOVA F-value for the provided sample.\n",
    "from sklearn.feature_selection import f_classif\n",
    "f, p = f_classif(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_ftest = pd.DataFrame(p, columns=['p-val'])\n",
    "pval_ftest.index = d[var_to_keep].iloc[:,1:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pval_ftest.style.set_table_attributes('style=\"font-size: 15px\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_ftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_ftest[pval_ftest['p-val']<.16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_indices = np.arange(x_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Univariate feature selection with F-test for feature scoring\n",
    "# We use the default selection function to select the four\n",
    "# most significant features\n",
    "\n",
    "selector = SelectKBest(f_classif, k=7)\n",
    "selector.fit(x_train, y_train)\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "scores /= scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.bar(X_indices - .45, scores, width=.8,\n",
    "        label=r'Univariate score ($-Log(p_{value})$)')\n",
    "plt.xlabel('Features')\n",
    "plt.xticks(X_indices, d[var_to_keep].iloc[:,1:].columns, rotation=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA feature selection for numeric input and categorical output\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# define feature selection\n",
    "fs = SelectKBest(score_func=f_classif, k=8)\n",
    "# apply feature selection\n",
    "x_selected = fs.fit_transform(x_train, y_train)\n",
    "print(x_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-islam",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-karaoke",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-capital",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "surprised-holiday",
   "metadata": {},
   "source": [
    "# RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report which features were selected by RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# load data\n",
    "d = pd.read_csv('source/d_num.csv')\n",
    "d = d.values\n",
    "x = d[:,1:]\n",
    "y = d[:,:1].ravel()\n",
    "\n",
    "# split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1, shuffle=True, stratify=y)\n",
    "\n",
    "# define RFE\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=8)\n",
    "\n",
    "# fit RFE\n",
    "rfe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('source/d_num.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_RFE = pd.DataFrame(zip(d.columns[1:], rfe.ranking_), columns=['Variable', 'Ranking']).sort_values(by='Ranking').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_RFE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
