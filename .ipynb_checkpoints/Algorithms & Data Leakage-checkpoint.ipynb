{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minor-definition",
   "metadata": {},
   "source": [
    "## **Linear Algorithms**\n",
    "\n",
    "* Logistic Regression\n",
    "* Linear Discriminant Analysis\n",
    "* Naive Bayes\n",
    "\n",
    "Linear algorithms are those that are often drawn from the field of statistics and make strong assumptions about the functional form of the problem. We can refer to them as linear because the output is a linear combination of the inputs, or weighted inputs, although this definition is stretched. You might also refer to these algorithms as probabilistic algorithms as they are often fit under a probabilistic framework. They are often fast to train and often perform very well. Examples of linear algorithms you should consider trying include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-study",
   "metadata": {},
   "source": [
    "## **Nonlinear Algorithms**\n",
    "\n",
    "Nonlinear algorithms are drawn from the field of machine learning and make few assumptions about the functional form of the problem. We can refer to them as nonlinear because the output is often a nonlinear mapping of inputs to outputs. They often require more data than linear algorithms and are slower to train. Examples of nonlinear algorithms you should consider trying include:\n",
    "\n",
    "* Decision Tree\n",
    "* k-Nearest Neighbors\n",
    "* Artificial Neural Networks\n",
    "* Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-vienna",
   "metadata": {},
   "source": [
    "## **Ensemble Algorithms**\n",
    "\n",
    "Ensemble algorithms are also drawn from the field of machine learning and combine the predictions from two or more models. There are many ensemble algorithms to choose from, but when spot-checking algorithms, it is a good idea to focus on ensembles of decision tree algorithms, given that they are known to perform so well in practice on a wide range of problems. Examples of ensembles of decision tree algorithms you should consider trying include:\n",
    "\n",
    "* Bagged Decision Trees\n",
    "* Random Forest\n",
    "* Extra Trees\n",
    "* Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-grenada",
   "metadata": {},
   "source": [
    "## **Summary**\n",
    "\n",
    "* Naive Algorithms\n",
    "    * Majority Class\n",
    "    * Minority Class\n",
    "    * Class Priors\n",
    "* Linear Algorithms\n",
    "    * Logistic Regression\n",
    "    * Linear Discriminant Analysis\n",
    "    * Naive Bayes\n",
    "* Nonlinear Algorithms\n",
    "    * Decision Tree\n",
    "    * k-Nearest Neighbors\n",
    "    * Artificial Neural Networks\n",
    "    * Support Vector Machine\n",
    "* Ensemble Algorithms\n",
    "    * Bagged Decision Trees\n",
    "    * Random Forest\n",
    "    * Extra Trees\n",
    "    * Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-nurse",
   "metadata": {},
   "source": [
    "# Spot Check Imbalanced Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-nation",
   "metadata": {},
   "source": [
    "There are perhaps four types of imbalanced classification techniques to spot check:\n",
    "\n",
    "Data Sampling Algorithms\n",
    "Cost-Sensitive Algorithms\n",
    "One-Class Algorithms\n",
    "Probability Tuning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-egypt",
   "metadata": {},
   "source": [
    "Examples of popular combinations of over and undersampling include:\n",
    "\n",
    "SMOTE and Random Undersampling\n",
    "SMOTE and Tomek Links\n",
    "SMOTE and Edited Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-limitation",
   "metadata": {},
   "source": [
    "## Cost-Sensitive Algorithms\n",
    "Cost-sensitive algorithms are modified versions of machine learning algorithms designed to take the differing costs of misclassification into account when fitting the model on the training dataset.\n",
    "\n",
    "These algorithms can be effective when used on imbalanced classification, where the cost of misclassification is configured to be inversely proportional to the distribution of examples in the training dataset.\n",
    "\n",
    "There are many cost-sensitive algorithms to choose from, although it might be practical to test a range of cost-sensitive versions of linear, nonlinear, and ensemble algorithms.\n",
    "\n",
    "Some examples of machine learning algorithms that can be configured using cost-sensitive training include:\n",
    "\n",
    "Logistic Regression\n",
    "Decision Trees\n",
    "Support Vector Machines\n",
    "Artificial Neural Networks\n",
    "Bagged Decision Trees\n",
    "Random Forest\n",
    "Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-calcium",
   "metadata": {},
   "source": [
    "## One-Class Algorithms\n",
    "\n",
    "Algorithms used for outlier detection and anomaly detection can be used for classification tasks.\n",
    "\n",
    "Although unusual, when used in this way, they are often referred to as one-class classification algorithms.\n",
    "\n",
    "In some cases, one-class classification algorithms can be very effective, such as when there is a severe class imbalance with very few examples of the positive class.\n",
    "\n",
    "Examples of one-class classification algorithms to try include:\n",
    "\n",
    "One-Class Support Vector Machines\n",
    "Isolation Forests\n",
    "Minimum Covariance Determinant\n",
    "Local Outlier Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-welding",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-session",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning\n",
    "After spot-checking machine learning algorithms and imbalanced algorithms, you will have some idea of what works and what does not on your specific dataset.\n",
    "\n",
    "The simplest approach to hyperparameter tuning is to select the top five or 10 algorithms or algorithm combinations that performed well and tune the hyperparameters for each.\n",
    "\n",
    "There are three popular hyperparameter tuning algorithms that you may choose from:\n",
    "\n",
    "Random Search\n",
    "Grid Search\n",
    "Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-compensation",
   "metadata": {},
   "source": [
    "# How to Fix k-Fold Cross-Validation for Imbalanced Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-healing",
   "metadata": {},
   "source": [
    "[Source: How to Fix k-Fold Cross-Validation for Imbalanced Classification, *Machine Learning Mastery*](https://machinelearningmastery.com/cross-validation-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-registrar",
   "metadata": {},
   "source": [
    "## Repeated Stratified K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "persistent-cheat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      ">Train: 0=660, 1=6, Test: 0=330, 1=4\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=6, Test: 0=330, 1=4\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=6, Test: 0=330, 1=4\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=6, Test: 0=330, 1=4\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=6, Test: 0=330, 1=4\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=6, Test: 0=330, 1=4\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=6, Test: 0=330, 1=4\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=6, Test: 0=330, 1=4\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=6, Test: 0=330, 1=4\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=6, Test: 0=330, 1=4\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n",
      ">Train: 0=660, 1=7, Test: 0=330, 1=3\n"
     ]
    }
   ],
   "source": [
    "# example of stratified k-fold cross-validation with an imbalanced dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.99, 0.01], flip_y=0, random_state=1)\n",
    "kfold = RepeatedStratifiedKFold(n_splits=3, random_state=1)\n",
    "# print out the number of splits created\n",
    "c = 0\n",
    "for train_ix, test_ix in kfold.split(X, y):\n",
    "    c += 1\n",
    "print(c)\n",
    "# enumerate the splits and summarize the distributions\n",
    "for train_ix, test_ix in kfold.split(X, y):\n",
    "\t# select rows\n",
    "\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
    "\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
    "\t# summarize train and test composition\n",
    "\ttrain_0, train_1 = len(train_y[train_y==0]), len(train_y[train_y==1])\n",
    "\ttest_0, test_1 = len(test_y[test_y==0]), len(test_y[test_y==1])\n",
    "\tprint('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-multiple",
   "metadata": {},
   "source": [
    "## Stratified K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lightweight-float",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Train: 0=792, 1=8, Test: 0=198, 1=2\n",
      ">Train: 0=792, 1=8, Test: 0=198, 1=2\n",
      ">Train: 0=792, 1=8, Test: 0=198, 1=2\n",
      ">Train: 0=792, 1=8, Test: 0=198, 1=2\n",
      ">Train: 0=792, 1=8, Test: 0=198, 1=2\n"
     ]
    }
   ],
   "source": [
    "# example of stratified k-fold cross-validation with an imbalanced dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.99, 0.01], flip_y=0, random_state=1)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# enumerate the splits and summarize the distributions\n",
    "for train_ix, test_ix in kfold.split(X, y):\n",
    "\t# select rows\n",
    "\ttrain_X, test_X = X[train_ix], X[test_ix]\n",
    "\ttrain_y, test_y = y[train_ix], y[test_ix]\n",
    "\t# summarize train and test composition\n",
    "\ttrain_0, train_1 = len(train_y[train_y==0]), len(train_y[train_y==1])\n",
    "\ttest_0, test_1 = len(test_y[test_y==0]), len(test_y[test_y==1])\n",
    "\tprint('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-damage",
   "metadata": {},
   "source": [
    "## Train-Test Split Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "illegal-agreement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Train: 0=495, 1=5, Test: 0=495, 1=5\n"
     ]
    }
   ],
   "source": [
    "# example of stratified train/test split with an imbalanced dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.99, 0.01], flip_y=0, random_state=1)\n",
    "# split into train/test sets with same class ratio\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2, stratify=y)\n",
    "# summarize\n",
    "train_0, train_1 = len(trainy[trainy==0]), len(trainy[trainy==1])\n",
    "test_0, test_1 = len(testy[testy==0]), len(testy[testy==1])\n",
    "print('>Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-contractor",
   "metadata": {},
   "source": [
    "# Data Leakage Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-seating",
   "metadata": {},
   "source": [
    "[Source: Stratified K Fold Cross Validation, *Geeks for Geeks*](https://www.geeksforgeeks.org/stratified-k-fold-cross-validation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-loading",
   "metadata": {},
   "source": [
    "## Repeated Stratified K-Fold Cross-Validation - WITH DATA LEAKAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "assigned-bristol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of possible accuracy: [0.93, 0.96, 0.98, 1.0, 0.96, 0.96, 0.98, 0.95, 0.95, 0.98]\n",
      "\n",
      "Maximum Accuracy That can be obtained from this model is: 100.0 %\n",
      "\n",
      "Minimum Accuracy: 93.0 %\n",
      "\n",
      "Overall Accuracy: 96.5 %\n",
      "\n",
      "Standard Deviation is: 0.02\n"
     ]
    }
   ],
   "source": [
    "# This code may not be run on GFG IDE\n",
    "# as required packages are not found.\n",
    "\t\n",
    "# STRATIFIES K-FOLD CROSS VALIDATION { 10-fold }\n",
    "\n",
    "# Import Required Modules.\n",
    "from statistics import mean, stdev\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "\n",
    "# FEATCHING FEATURES AND TARGET VARIABLES IN ARRAY FORMAT.\n",
    "cancer = datasets.load_breast_cancer()\n",
    "\n",
    "# Input_x_Features.\n",
    "x = cancer.data\n",
    "\n",
    "# Input_ y_Target_Variable.\n",
    "y = cancer.target\n",
    "\n",
    "# Feature Scaling for input features.\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# Create classifier object.\n",
    "lr = linear_model.LogisticRegression()\n",
    "\n",
    "# Create StratifiedKFold object.\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "lst_accu_stratified = []\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "    lr.fit(x_train_fold, y_train_fold)\n",
    "    lst_accu_stratified.append(round(lr.score(x_test_fold, y_test_fold),2))\n",
    "\n",
    "# Print the output.\n",
    "print('List of possible accuracy:', lst_accu_stratified)\n",
    "print('\\nMaximum Accuracy That can be obtained from this model is:',\n",
    "\tround(max(lst_accu_stratified)*100,2), '%')\n",
    "print('\\nMinimum Accuracy:',\n",
    "\tround(min(lst_accu_stratified)*100,2), '%')\n",
    "print('\\nOverall Accuracy:',\n",
    "\tround(mean(lst_accu_stratified)*100,2), '%')\n",
    "print('\\nStandard Deviation is:', round(stdev(lst_accu_stratified),2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-coordinate",
   "metadata": {},
   "source": [
    "## Repeated Stratified K-Fold Cross-Validation - WITHOUT DATA LEAKAGE (fixed by myself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "anticipated-passenger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of possible accuracy: [0.93, 0.96, 0.98, 0.98, 0.96, 0.96, 0.98, 0.95, 0.95, 0.98]\n",
      "\n",
      "Maximum Accuracy That can be obtained from this model is: 98.0 %\n",
      "\n",
      "Minimum Accuracy: 93.0 %\n",
      "\n",
      "Overall Accuracy: 96.3 %\n",
      "\n",
      "Standard Deviation is: 0.02\n"
     ]
    }
   ],
   "source": [
    "# This code may not be run on GFG IDE\n",
    "# as required packages are not found.\n",
    "\t\n",
    "# STRATIFIES K-FOLD CROSS VALIDATION { 10-fold }\n",
    "\n",
    "# Import Required Modules.\n",
    "from statistics import mean, stdev\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import linear_model\n",
    "from sklearn import datasets\n",
    "\n",
    "# FEATCHING FEATURES AND TARGET VARIABLES IN ARRAY FORMAT.\n",
    "cancer = datasets.load_breast_cancer()\n",
    "\n",
    "# Input_x_Features.\n",
    "x = cancer.data\n",
    "\n",
    "# Input_ y_Target_Variable.\n",
    "y = cancer.target\n",
    "\n",
    "# Create classifier object.\n",
    "lr = linear_model.LogisticRegression()\n",
    "\n",
    "# Create StratifiedKFold object.\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "lst_accu_stratified = []\n",
    "\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    x_train_fold, x_test_fold = x_scaled[train_index], x_scaled[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "    \n",
    "    # Feature Scaling for input features.\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    scaler.fit(x_train_fold)\n",
    "    x_train_scaled = scaler.transform(x_train_fold)\n",
    "    x_test_scaled = scaler.transform(x_test_fold)\n",
    "    \n",
    "    lr.fit(x_train_scaled, y_train_fold)\n",
    "    lst_accu_stratified.append(round(lr.score(x_test_scaled, y_test_fold),2))\n",
    "\n",
    "# Print the output.\n",
    "print('List of possible accuracy:', lst_accu_stratified)\n",
    "print('\\nMaximum Accuracy That can be obtained from this model is:',\n",
    "\tround(max(lst_accu_stratified)*100,2), '%')\n",
    "print('\\nMinimum Accuracy:',\n",
    "\tround(min(lst_accu_stratified)*100,2), '%')\n",
    "print('\\nOverall Accuracy:',\n",
    "\tround(mean(lst_accu_stratified)*100,2), '%')\n",
    "print('\\nStandard Deviation is:', round(stdev(lst_accu_stratified),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-retail",
   "metadata": {},
   "source": [
    "# Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-castle",
   "metadata": {},
   "source": [
    "[Source: How to Avoid Data Leakage When Performing Data Preparation, *Machine Learning Mastery*](https://machinelearningmastery.com/data-preparation-without-data-leakage/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-minimum",
   "metadata": {},
   "source": [
    "## With Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "corresponding-retrieval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.848\n"
     ]
    }
   ],
   "source": [
    "# naive approach to normalizing the data before splitting the data and evaluating the model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# standardize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-warning",
   "metadata": {},
   "source": [
    "## Without Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "metric-guest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.455\n"
     ]
    }
   ],
   "source": [
    "# correct approach for normalizing the data after the data is split before the model is evaluated\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit on the training dataset\n",
    "scaler.fit(X_train)\n",
    "# scale the training dataset\n",
    "X_train = scaler.transform(X_train)\n",
    "# scale the test dataset\n",
    "X_test = scaler.transform(X_test)\n",
    "# fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
