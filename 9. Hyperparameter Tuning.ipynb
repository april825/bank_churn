{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hired-secretary",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:rgb(67, 77, 86);\n",
    "           font-size:300%;\n",
    "           font-style: oblique;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 20px;\">Predicting Bank Churners</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-correspondence",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "<h2 style=\"background-color:rgb(141, 153, 165);\n",
    "           font-size:250%;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 10px;\">Chapter 5. Spot Check Version 1</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-controversy",
   "metadata": {},
   "source": [
    "<a id='1.1'>\n",
    "    <h2 style='font-size:180%;'>\n",
    "        Mission</h2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-azerbaijan",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <blockquote cite='https://www.kaggle.com/sakshigoyal7/credit-card-customers/tasks?taskId=2729'>\n",
    "        <p style='font-size:110%;\n",
    "                  color:hsl(208, 12%, 30%);'><i>Our top priority in this business problem is to identify customers who are getting churned. Even if we predict non-churning customers as churned, it won't harm our business. But predicting churning customers as non-churning will do. So recall needs to be higher. Till now, I have managed to get a recall of 62%.</i></p>\n",
    "    </blockquote>\n",
    "    <figcaption>â€”Sakshi Goyal, <cite>Credit Card Customers, Kaggle</cite></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-wings",
   "metadata": {},
   "source": [
    "<a id='4.1'>\n",
    "    <h2 style='font-size:180%;'>\n",
    "        Libraries</h2></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pediatric-yellow",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# binary classification spot check script\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "harmful-dollar",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.set_printoptions(suppress=True, precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ethical-oklahoma",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "/* CSS styles for pandas dataframe */\n",
       ".dataframe th {\n",
       "    font-size: 16px;\n",
       "}\n",
       ".dataframe td {\n",
       "    font-size: 14px;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "/* CSS styles for pandas dataframe */\n",
    ".dataframe th {\n",
    "    font-size: 16px;\n",
    "}\n",
    ".dataframe td {\n",
    "    font-size: 14px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "close-vehicle",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-pharmaceutical",
   "metadata": {},
   "source": [
    "<a id=\"1.2\"></a>\n",
    "<h2 style=\"background-color:rgb(141, 153, 165);\n",
    "           font-size:250%;\n",
    "           color:white;\n",
    "           text-align:center;\n",
    "           margin: auto;\n",
    "           padding: 10px;\">Spot Check for Model & Scaler</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-biology",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-nomination",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-catholic",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-capacity",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perf_metrics(y_test, y_pred):\n",
    "    dic = {}\n",
    "    dic['accuracy'] = round(accuracy_score(y_test, y_pred), 2)\n",
    "    dic['precision'] = round(precision_score(y_test, y_pred), 2)\n",
    "    dic['recall'] = round(recall_score(y_test, y_pred), 2)\n",
    "    dic['f1'] = round(f1_score(y_test, y_pred), 2)\n",
    "    dic['f2'] = round(fbeta_score(y_test, y_pred, beta=2), 2)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-skill",
   "metadata": {},
   "source": [
    "### Result Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-alaska",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def result_rskf(x, y, pipeline, mod_disp_name, n_splits=5, n_repeats=3):\n",
    "   \n",
    "    # define cv method\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=n_splits, n_repeats=n_repeats, random_state=1)  \n",
    "    \n",
    "    # define performance metrics\n",
    "    scoring = {\n",
    "        'accuracy':'accuracy', 'precision':'precision', 'recall':'recall', 'f1':'f1', \n",
    "        'f2':make_scorer(fbeta_score, beta=2)} # dict val = scorer fct or predefined metric str  \n",
    "    \n",
    "    # evaluate result\n",
    "    result = cross_validate(\n",
    "        pipeline, x, y, cv=cv, \n",
    "        scoring=scoring, return_train_score=True, n_jobs=-1)\n",
    "        \n",
    "    # make a summary table\n",
    "    df = pd.DataFrame(\n",
    "        (k, mean(v), std(v)) for k,v in result.items()\n",
    "        ).rename({0:'metric', 1:'mean', 2:'std'}, axis=1\n",
    "                ).set_index('metric')\n",
    "    df.index.name = None\n",
    "    df.columns = pd.MultiIndex.from_product([[mod_disp_name],df.columns])\n",
    "    \n",
    "    return df, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-reservation",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def result_tts(x, y, pipeline, mod_disp_name):\n",
    "    \n",
    "    # define cv method\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.2, random_state=1, shuffle=True, stratify=y)\n",
    "    \n",
    "    # evaluate result   \n",
    "    time_0 = time.time()\n",
    "    pipeline.fit(x_train, y_train)\n",
    "    time_1 = time.time()\n",
    "    y_pred = pipeline.predict(x_test)\n",
    "    time_2 = time.time()\n",
    "    result = {}\n",
    "    result['fit_time'] = round(time_1-time_0, 2)\n",
    "    result['score_time'] = round(time_2-time_1, 2)\n",
    "    result['accuracy'] = round(accuracy_score(y_test, y_pred), 2)\n",
    "    result['precision'] = round(precision_score(y_test, y_pred), 2)\n",
    "    result['recall'] = round(recall_score(y_test, y_pred), 2)\n",
    "    result['f1'] = round(f1_score(y_test, y_pred), 2)\n",
    "    result['f2'] = round(fbeta_score(y_test, y_pred, beta=2), 2)\n",
    "    conf_mat = confusion_matrix(y_test, y_pred, labels=[1,0])\n",
    "    \n",
    "    # make a summary table    \n",
    "    df = pd.DataFrame(result, index=[mod_disp_name]).T\n",
    "    \n",
    "    return df, result, conf_mat, y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-apparel",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summary_by_mod(x, y, models, scalers, result_func=result_rskf, **n_splits_and_repeats):\n",
    "    results = []\n",
    "    time_0 = time.time() # for all methods in pipeline\n",
    "    for scaler in scalers:\n",
    "        results_models = []\n",
    "        time_1 = time.time() # for each scaler\n",
    "        print(f'Scaler: {scaler[0]}\\n')\n",
    "        for model in models:\n",
    "            time_2 = time.time() # for each model\n",
    "            pipeline = Pipeline([('s', scaler[1]), ('m', model[1])])\n",
    "            if result_func==result_rskf:\n",
    "                n_splits, n_repeats = (i for i in n_splits_and_repeats.values())\n",
    "                results_model = result_func(x, y, pipeline, model[0], n_splits, n_repeats)[0]\n",
    "            else:\n",
    "                results_model = result_func(x, y, pipeline, model[0])[0]\n",
    "            print(f'Model {model[0]} Runtime: {time.strftime(\"%M:%S\", time.gmtime(time.time()-time_2))}')\n",
    "            results_models.append(results_model)\n",
    "        print(f'Scaler {scaler[0]} Avg Runtime per Model: {time.strftime(\"%M:%S\", time.gmtime((time.time()-time_1)/len(models)))}\\n\\n')\n",
    "        results.append(results_models)\n",
    "    print(f'Total Runtime: {time.strftime(\"%M:%S\", time.gmtime(time.time()-time_0))} min')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-bermuda",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-trainer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(X, y, model, resampler, folds, metric, pipe_func):\n",
    "    \n",
    "    # create the pipeline\n",
    "    pipeline = pipe_func(model, resampler)\n",
    "    \n",
    "    # define cv method\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=10, n_repeats=5, random_state=5)  \n",
    "    \n",
    "    # define performance metrics\n",
    "    scoring = {\n",
    "        'accuracy':'accuracy', 'precision':'precision', 'recall':'recall', 'f1':'f1', \n",
    "        'f2':make_scorer(fbeta_score, beta=2)} # dict val = scorer fct or predefined metric str  \n",
    "    \n",
    "    # evaluate result\n",
    "    scores = cross_validate(\n",
    "        pipeline, x, y, cv=cv, \n",
    "        scoring=scoring, return_train_score=True, n_jobs=-1)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "# evaluate a model and try to trap errors and and hide warnings\n",
    "def robust_evaluate_model(X, y, model, resampler, folds, metric, pipe_func):\n",
    "    scores = None\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            scores = evaluate_model(X, y, model, resampler, folds, metric, pipe_func)\n",
    "    except:\n",
    "        scores = None\n",
    "    return scores\n",
    "\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(X, y, models, resamplers, pipe_funcs, folds=10, metric='recall'):\n",
    "    results = dict()\n",
    "    for name_model, model in models:\n",
    "        for name_resampler, resampler in resamplers:\n",
    "            # evaluate model under each preparation function\n",
    "            for i in range(len(pipe_funcs)):\n",
    "                # evaluate the model\n",
    "                scores = robust_evaluate_model(X, y, model, resampler, folds, metric, pipe_funcs[i])\n",
    "                # update name\n",
    "                run_name = str(i) + '_' + name_model + '_' + name_resampler\n",
    "                # show process\n",
    "                if scores is not None:\n",
    "                    # store a result\n",
    "                    results[run_name] = scores\n",
    "                    mean_score, std_score = np.nanmean(scores), np.nanstd(scores)\n",
    "                    print('>%s: %.3f (+/-%.3f)' % (run_name, mean_score, std_score))\n",
    "                else:\n",
    "                    print('>%s: error' % run_name)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-cologne",
   "metadata": {},
   "source": [
    "## Examine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-player",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print and plot the top n results\n",
    "def summarize_results(results, maximize=True, top_n=10):\n",
    "\t# check for no results\n",
    "\tif len(results) == 0:\n",
    "\t\tprint('no results')\n",
    "\t\treturn\n",
    "\t# determine how many results to summarize\n",
    "\tn = min(top_n, len(results))\n",
    "\t# create a list of (name, mean(scores)) tuples\n",
    "\tmean_scores = [(k, np.nanmean(v)) for k,v in results.items()]\n",
    "\t# sort tuples by mean score\n",
    "\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "\t# reverse for descending order (e.g. for accuracy)\n",
    "\tif maximize:\n",
    "\t\tmean_scores = list(reversed(mean_scores))\n",
    "\t# retrieve the top n for summarization\n",
    "\tnames = [x[0] for x in mean_scores[:n]]\n",
    "\tscores = [results[x[0]] for x in mean_scores[:n]]\n",
    "\t# print the top n\n",
    "\tprint()\n",
    "\tfor i in range(n):\n",
    "\t\tname = names[i]\n",
    "\t\tmean_score, std_score = np.nanmean(results[name]), np.nanstd(results[name])\n",
    "\t\tprint('Rank=%d, Name=%s, Score=%.3f (+/- %.3f)' % (i+1, name, mean_score, std_score))\n",
    "\t# boxplot for the top n\n",
    "\tpyplot.boxplot(scores, labels=names)\n",
    "\t_, labels = pyplot.xticks()\n",
    "\tpyplot.setp(labels, rotation=90)\n",
    "\tpyplot.savefig('spotcheck.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-sense",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-blade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-report",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-cancellation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "buried-square",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the dataset, returns X and y elements\n",
    "def load_dataset():\n",
    "    d = pd.read_csv('source/d_num.csv')\n",
    "    d_values = d.values\n",
    "    x, y = d_values[:,1:], d_values[:,:1].ravel()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "empty-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "logit = LogisticRegression(random_state=5)\n",
    "ada = AdaBoostClassifier(random_state=5)\n",
    "gb = GradientBoostingClassifier(random_state=5)\n",
    "\n",
    "# scalers\n",
    "rs = ('RS', RobustScaler())\n",
    "qs = ('QT', QuantileTransformer())\n",
    "\n",
    "def pipeline_scaler_ENN(scaler, model):\n",
    "    steps = list()\n",
    "    # normalization\n",
    "    steps.append((scaler[0], scaler[1]))\n",
    "    # standardization\n",
    "    steps.append(('SS', StandardScaler()))\n",
    "    # the resampler\n",
    "    steps.append(('RSP', SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))))\n",
    "    # the model\n",
    "    steps.append(('MOD', model))\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "molecular-participant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('QT', QuantileTransformer()), ('SS', StandardScaler()),\n",
       "                ('RSP',\n",
       "                 SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))),\n",
       "                ('MOD', GradientBoostingClassifier(random_state=5))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_scaler_ENN(qs, gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = pd.read_csv('source/d_num.csv')\n",
    "# d_values = d.values\n",
    "# x, y = d_values[:,1:], d_values[:,:1].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "likely-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = pd.read_csv('source/d_num.csv')\n",
    "# X, y = d.iloc[:,1:], d.iloc[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-baptist",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Define a pipeline to search for the best combination of PCA truncation\n",
    "# and classifier regularization.\n",
    "pca = PCA()\n",
    "# set the tolerance to a large value to make the example faster\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# Parameters of pipelines can be set using â€˜__â€™ separated parameter names:\n",
    "param_grid = {\n",
    "    'pca__n_components': [5, 15, 30, 45, 64],\n",
    "    'logistic__C': np.logspace(-4, 4, 4),\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "search.fit(X_digits, y_digits)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n",
    "# Plot the PCA spectrum\n",
    "pca.fit(X_digits)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "ax0.plot(np.arange(1, pca.n_components_ + 1),\n",
    "         pca.explained_variance_ratio_, '+', linewidth=2)\n",
    "ax0.set_ylabel('PCA explained variance ratio')\n",
    "\n",
    "ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "ax0.legend(prop=dict(size=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Use RandomSearchCV to tune parameters for RFC \n",
    "\n",
    "# Takes a while to compute and uses a lot of CPU \n",
    "# https://stats.stackexchange.com/questions/186182/a-way-to-maintain-classifiers-recall-while-improving-precision\n",
    "#model\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "print('Running RandomizedSearchCV')\n",
    "# default RFClassifier but set random_state = 0 to keep consistent results\n",
    "MOD = RandomForestClassifier(random_state=0) \n",
    "#Implement RandomSearchCV\n",
    "# Number of trees in random forest [100,150,...,500]\n",
    "n_estimators = [int(x) for x in np.arange(start = 100, stop = 501, step = 50)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.arange(start = 20, stop = 101, step = 20)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "scoreFunction = {\"recall\": \"recall\"}\n",
    "# run a RandomizedSearchCV with 3 folds and 25 iterations \n",
    "random_search = RandomizedSearchCV(MOD,\n",
    "                                   param_distributions = random_grid,\n",
    "                                   n_iter = 25,\n",
    "                                   scoring = scoreFunction,               \n",
    "                                   refit = \"recall\",\n",
    "                                   return_train_score = False,\n",
    "                                   random_state = 0,\n",
    "                                   verbose = 2,\n",
    "                                   cv = 3,\n",
    "                                   n_jobs = -1) \n",
    "\n",
    "#trains and optimizes the model\n",
    "random_search.fit(X_train80, y_train80)\n",
    "\n",
    "print('Finished RandomizedSearchCV ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of grid searching key hyperparameters for GradientBoostingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# define dataset\n",
    "X, y = load_dataset()\n",
    "# define models and parameters\n",
    "model = pipeline_scaler_ENN(qs, gb)\n",
    "n_estimators = [100, 300, 800]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.7, 1.0]\n",
    "max_depth = [3, 7, 9]\n",
    "# define grid search\n",
    "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='recall', error_score=0)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-pension",
   "metadata": {},
   "source": [
    "## Run Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-association",
   "metadata": {},
   "source": [
    "Across all models, the SMOTE oversampling method in conjunction with the Edited Nearest Neighbor undersampling performed the best. Among the predictive algorithms, GB, ADA, and LR look promising. In the next session, we will narrow down our models and jump into hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-alexandria",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "X, y = load_dataset()\n",
    "# get model list\n",
    "models = define_models()\n",
    "# get model list\n",
    "resamplers = define_resamplers()\n",
    "# define transform pipelines\n",
    "pipelines = [pipeline_QTSS_ENN, pipeline_RSSS_ENN]\n",
    "# evaluate models\n",
    "results = evaluate_models(X, y, models, resamplers, pipelines)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-guide",
   "metadata": {},
   "source": [
    "## Calculate Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_1 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Finished in {round(time_1-time_0, 2):,} second(s) or {round((time_1-time_0)/60, 2)} minute(s).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank=1, Name=2_GB_800_SM_ENN, Score=0.914 (+/- 0.063)\n",
    "# Rank=2, Name=2_ADA_800_SM_ENN, Score=0.910 (+/- 0.056)\n",
    "# Rank=3, Name=1_GB_800_SM_ENN, Score=0.908 (+/- 0.087)\n",
    "# Rank=4, Name=1_ADA_800_SM_ENN, Score=0.906 (+/- 0.063)\n",
    "# Rank=5, Name=1_LR_SM_ENN, Score=0.870 (+/- 0.107)\n",
    "# Rank=6, Name=2_LR_SM_ENN, Score=0.868 (+/- 0.075)\n",
    "# Rank=7, Name=2_MLP_1000_SM_ENN, Score=0.867 (+/- 0.078)\n",
    "# Rank=8, Name=1_GB_800_SM_TM, Score=0.859 (+/- 0.099)\n",
    "# Rank=9, Name=2_GB_800_SM, Score=0.857 (+/- 0.099)\n",
    "# Rank=10, Name=1_GB_800_SM, Score=0.857 (+/- 0.096)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
